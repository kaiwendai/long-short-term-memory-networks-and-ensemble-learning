{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Long-Short Term Memory Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will implement a long-short term memory (LSTM) models from scratch and train them for the language modeling task. We will also do some error analysis and ablation studies on the learnt model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some helper functions to handle the text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some hyper-parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "embed_size = 32         # size of the input feature vector representing each word\n",
    "hidden_size = 32        # number of hidden units in the LSTM cell\n",
    "num_epochs = 1          # number of epochs for which you will train your model    \n",
    "num_samples = 200       # number of words to be sampled\n",
    "batch_size = 20         # the size of your mini-batch\n",
    "seq_length = 30         # the size of the BPTT window\n",
    "learning_rate = 0.002   # learning rate of the model\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the sequential data \n",
    "\n",
    "We will use the Penn Tree Bank dataset for the purpose of this exercise. We need to download by running the command `wget https://data.deepai.org/ptbdataset.zip`, unzip it, and store it in the directory `./data/ptb`. We will only use the files `ptb.train.txt` and `ptb.test.txt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1549\n",
      "137\n",
      "tensor([[   0,    1,    2,  ...,  152, 4955, 4150],\n",
      "        [  93,  718,  590,  ...,  170, 6784,  133],\n",
      "        [  27,  930,   42,  ...,  392, 4864,   26],\n",
      "        ...,\n",
      "        [ 997,   42,  507,  ...,  682, 6849, 6344],\n",
      "        [ 392, 5518, 3034,  ..., 2264,   42, 3401],\n",
      "        [4210,  467, 1496,  ..., 9999,  119, 1143]])\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "corpus = Corpus()\n",
    "ids = corpus.get_data('./data/ptb/ptb.train.txt', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length\n",
    "\n",
    "test_corpus = Corpus()\n",
    "test_ids = test_corpus.get_data('./data/ptb/ptb.test.txt', batch_size)\n",
    "test_vocab_size =  len(test_corpus.dictionary)\n",
    "test_num_batches = test_ids.size(1) // seq_length\n",
    "\n",
    "print(num_batches)\n",
    "print(test_num_batches)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Long-Short Term Memory (LSTM) unit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single LSTM unit performs the following operations: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "i_t & = \\sigma (W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "f_t & = \\sigma (W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "g_t & = \\sigma (W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "o_t & = \\sigma (W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "c_t & = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "h_t & = o_t \\odot \\tanh(c_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is called the memory cell state at time $t$, $x_t$ is the input at time $t$, $h_{t-1}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$, are the input, forget, cell, and output gates, respectively. $\\sigma$ is the sigmoid function, and $\\odot$ is the Hadamard (element-wise) product of two vectors. Finally $\\{W_{ii}, b_{ii}, W_{hi}, b_{hi}\\}$, $\\{W_{if}, b_{if}, W_{hf}, b_{hf}\\}$, $\\{W_{ig}, b_{ig}, W_{hg}, b_{hg}\\}$, and $\\{W_{io}, b_{io}, W_{ho}, b_{ho}\\}$ are the learnable weights and biases for computing the input, forget, cell, and output gates respectively.\n",
    "\n",
    "At each time step the LSTM takes as input the previous hidden state, the previous memory cell state and the embedding (features) associated with the current word and generates the new hidden states and the prediction of the next word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myLSTM based language model\n",
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Input: \n",
    "            vocab_size: the number of unique words in your dataset \n",
    "            embed_size: the size of the feature vector assiciated with each word\n",
    "            hidden_size: the size of the hidden state features\n",
    "        \"\"\"\n",
    "        super(myLSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size) # features associated with the words in your vocabulary\n",
    "        \n",
    "        # rest of your LSTM model code goes here\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        #i_t\n",
    "        self.Wii = nn.Parameter(torch.Tensor(embed_size, hidden_size))\n",
    "        self.bii = nn.Parameter(torch.Tensor(embed_size))\n",
    "        self.Whi = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.bhi = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        \n",
    "        #f_t\n",
    "        self.Wif = nn.Parameter(torch.Tensor(embed_size, hidden_size))\n",
    "        self.bif = nn.Parameter(torch.Tensor(embed_size))\n",
    "        self.Whf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.bhf = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        \n",
    "        #g_t\n",
    "        self.Wig = nn.Parameter(torch.Tensor(embed_size, hidden_size))\n",
    "        self.big = nn.Parameter(torch.Tensor(embed_size))\n",
    "        self.Whg = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.bhg = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        \n",
    "        #o_t\n",
    "        self.Wio = nn.Parameter(torch.Tensor(embed_size, hidden_size))\n",
    "        self.bio = nn.Parameter(torch.Tensor(embed_size))\n",
    "        self.Who = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.bho = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        \n",
    "        \n",
    "        self.linear = torch.nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        \"\"\"\n",
    "        Perform forward propagation over a single unit composed of an LSTM unit followed by a linear layer to \n",
    "        generate the input activations for the softmax function. \n",
    "    \n",
    "        Input:\n",
    "            x: the current input (indices into the vocabulary)\n",
    "            h: the previous hidden states\n",
    "\n",
    "        Output:\n",
    "            out: the output of the unit (which are the input activations of the softmax layer to predict the next word)\n",
    "            (h, c): the current hidden states composed of the hidden state and the memory cell state\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.embed(x)\n",
    "        batch_sz, seq_sz, embed_sz = x.size()\n",
    "        out = []\n",
    "        h_t, c_t = h\n",
    "        \n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            i_t = torch.sigmoid(x_t @ self.Wii +self.bii+ h_t @ self.Whi + self.bhi)\n",
    "            f_t = torch.sigmoid(x_t @ self.Wif +self.bif+ h_t @ self.Whf + self.bhf)\n",
    "            g_t = torch.sigmoid(x_t @ self.Wig +self.big+ h_t @ self.Whg + self.bhg)\n",
    "            o_t = torch.sigmoid(x_t @ self.Wio +self.bio+ h_t @ self.Who + self.bho)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            out.append(self.linear(o_t))\n",
    "            \n",
    "        out = torch.cat(out, dim=0)\n",
    "        out = out.transpose(0, 1).contiguous()\n",
    "        h = h_t\n",
    "        c = c_t\n",
    "        return out, (h, c)\n",
    "\n",
    "model = myLSTM(vocab_size, embed_size, hidden_size)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()     # the loss function\n",
    "\n",
    "# we will use the Adam optimizer for faster and easier convergence\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step[0/1549], Loss: 9.0651, Perplexity: 8647.89\n",
      "Epoch [1/1], Step[100/1549], Loss: 3.5338, Perplexity: 34.25\n",
      "Epoch [1/1], Step[200/1549], Loss: 3.5332, Perplexity: 34.23\n",
      "Epoch [1/1], Step[300/1549], Loss: 3.4413, Perplexity: 31.23\n",
      "Epoch [1/1], Step[400/1549], Loss: 3.3500, Perplexity: 28.50\n",
      "Epoch [1/1], Step[500/1549], Loss: 3.4548, Perplexity: 31.65\n",
      "Epoch [1/1], Step[600/1549], Loss: 3.4627, Perplexity: 31.90\n",
      "Epoch [1/1], Step[700/1549], Loss: 3.3832, Perplexity: 29.47\n",
      "Epoch [1/1], Step[800/1549], Loss: 3.4258, Perplexity: 30.75\n",
      "Epoch [1/1], Step[900/1549], Loss: 3.4313, Perplexity: 30.92\n",
      "Epoch [1/1], Step[1000/1549], Loss: 3.3553, Perplexity: 28.65\n",
      "Epoch [1/1], Step[1100/1549], Loss: 3.5255, Perplexity: 33.97\n",
      "Epoch [1/1], Step[1200/1549], Loss: 3.4481, Perplexity: 31.44\n",
      "Epoch [1/1], Step[1300/1549], Loss: 3.4302, Perplexity: 30.88\n",
      "Epoch [1/1], Step[1400/1549], Loss: 3.4573, Perplexity: 31.73\n",
      "Epoch [1/1], Step[1500/1549], Loss: 3.4560, Perplexity: 31.69\n"
     ]
    }
   ],
   "source": [
    "# Truncated backpropagation\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states]\n",
    "\n",
    "np.random.seed(10003)\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    states = (torch.zeros(1, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(1, batch_size, hidden_size).to(device))\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        states = detach(states)\n",
    "        \n",
    "        ## the code for the rest of the forward pass goes here ##\n",
    "        output, states = model.forward(inputs, states)\n",
    "        \n",
    "        \n",
    "        # Backward pass \n",
    "        \n",
    "        ## code for backward pass goes here ##\n",
    "        output = output[:, -1, :]\n",
    "        targets = targets.argmax(dim=1)\n",
    "\n",
    "        loss = criterion(output, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # we clip the gradients to ensure that they remain bounded. This is a hack! \n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        # Optimize        \n",
    "        \n",
    "        ## code for optimization goes here ##\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing loop for the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss  3.39 | test ppl    29.67\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Test the model\n",
    "states = (torch.zeros(1, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(1, batch_size, hidden_size).to(device))\n",
    "test_loss = 0.\n",
    "with torch.no_grad():\n",
    "    for i in range(0, test_ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = test_ids[:, i:i+seq_length].to(device)\n",
    "        targets = test_ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        states = detach(states)\n",
    "        output, states = model.forward(inputs, states)\n",
    "        output = output[:, -1, :]\n",
    "        targets = targets.argmax(dim=1)\n",
    "        test_loss += criterion(output, targets)\n",
    "\n",
    "\n",
    "test_loss = test_loss / test_num_batches\n",
    "print('test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled [100/200] words and save to sample.txt\n",
      "Sampled [200/200] words and save to sample.txt\n"
     ]
    }
   ],
   "source": [
    "# Generate texts using trained model\n",
    "with torch.no_grad():\n",
    "    with open('sample.txt', 'w') as f:\n",
    "        # Set intial hidden ane cell states\n",
    "        state = (torch.zeros(1, 1, hidden_size).to(device),\n",
    "                 torch.zeros(1, 1, hidden_size).to(device))\n",
    "\n",
    "        # Select one word id randomly\n",
    "        prob = torch.ones(vocab_size)\n",
    "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # Forward propagate RNN \n",
    "            output, state = model(input, state)\n",
    "\n",
    "            # Sample a word id\n",
    "            prob = output.exp()\n",
    "            word_id = torch.multinomial(prob[0], num_samples=1).item()\n",
    "\n",
    "            # Fill input with sampled word id for the next time step\n",
    "            input.fill_(word_id)\n",
    "\n",
    "            # File write\n",
    "            word = corpus.dictionary.idx2word[word_id]\n",
    "            word = '\\n' if word == '<eos>' else word + ' '\n",
    "            f.write(word)\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Ensemble of Neural Networks \n",
    "In this part of the project, you will train an ensemble of neural networks for classifying hand written digits in the MNIST dataset. We will take the code provided in the following cell and extend it to build an ensemble of single hidden layer MLP as per the specifications provided in the questions below. \n",
    "\n",
    "For the data, we need to download the MNIST dataset from ``http://yann.lecun.com/exdb/mnist/``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a: Ensemble of networks of same size but initialized with a different seed \n",
    "We will create an ensemble of `k` single hidden layer MLP models, each initialized with a different random seed. Train each model inside the ensemble for exactly `1` epoch and report the performance of the individual models and the ensemble model on the test set. \n",
    "\n",
    "At a high level we are performing the following tasks:\n",
    "1. Initialize `k` models with different random seed (for some value of k): \n",
    "2. Implement train() function to train individual models inside the ensemble: \n",
    "3. Implement the test() function to estimate the accuracy of the individual models and the ensemble model: \n",
    "4. Repeat the above process with different values of `k`. For this exercise use $k = \\{1, 2, 4, 8, 16, 32 \\}$, and plot a graph with `k` on the x-axis and ensemble model performance on the y-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3210\n",
      "Model 1 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.564294\n",
      "Model 1 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.994002\n",
      "Model 1 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.070880\n",
      "Model 1 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.512004\n",
      "Model 1 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.480439\n",
      "Model 1 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.374933\n",
      "Model 1 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.318229\n",
      "Model 1 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.215306\n",
      "Model 1 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.329168\n",
      "Model 1 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.184551\n",
      "\n",
      "Model 1 Test set: Average loss: 1.2020, Accuracy: 5330/10000 (53%)\n",
      "\n",
      "Accuracy of Ensemble of networks with 1 models: 5330/10000 (53%)\n",
      "\n",
      "Number of parameters: 3210\n",
      "Model 1 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.572860\n",
      "Model 1 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.849522\n",
      "Model 1 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.875718\n",
      "Model 1 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.570389\n",
      "Model 1 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.507716\n",
      "Model 1 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.525678\n",
      "Model 1 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.483477\n",
      "Model 1 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.502414\n",
      "Model 1 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.435999\n",
      "Model 1 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.173107\n",
      "Number of parameters: 3210\n",
      "Model 2 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.418138\n",
      "Model 2 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.945113\n",
      "Model 2 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.654005\n",
      "Model 2 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.542156\n",
      "Model 2 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.418450\n",
      "Model 2 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.416927\n",
      "Model 2 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.220297\n",
      "Model 2 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.239289\n",
      "Model 2 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.967664\n",
      "Model 2 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.950231\n",
      "\n",
      "Model 1 Test set: Average loss: 1.2152, Accuracy: 5411/10000 (54%)\n",
      "\n",
      "\n",
      "Model 2 Test set: Average loss: 0.9705, Accuracy: 6953/10000 (70%)\n",
      "\n",
      "Accuracy of Ensemble of networks with 2 models: 6074/10000 (61%)\n",
      "\n",
      "Number of parameters: 3210\n",
      "Model 1 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.448848\n",
      "Model 1 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.135307\n",
      "Model 1 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.145172\n",
      "Model 1 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.914125\n",
      "Model 1 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.575818\n",
      "Model 1 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.523190\n",
      "Model 1 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.263305\n",
      "Model 1 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.374839\n",
      "Model 1 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.254715\n",
      "Model 1 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.030398\n",
      "Number of parameters: 3210\n",
      "Model 2 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.336092\n",
      "Model 2 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.835104\n",
      "Model 2 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.862002\n",
      "Model 2 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.258370\n",
      "Model 2 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.259969\n",
      "Model 2 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.977145\n",
      "Model 2 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.254328\n",
      "Model 2 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.824517\n",
      "Model 2 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.842640\n",
      "Model 2 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.969514\n",
      "Number of parameters: 3210\n",
      "Model 3 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.455434\n",
      "Model 3 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.828434\n",
      "Model 3 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.471585\n",
      "Model 3 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.438054\n",
      "Model 3 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.120611\n",
      "Model 3 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.138565\n",
      "Model 3 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.206863\n",
      "Model 3 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.151411\n",
      "Model 3 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.956106\n",
      "Model 3 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.919638\n",
      "Number of parameters: 3210\n",
      "Model 4 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.547161\n",
      "Model 4 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.106678\n",
      "Model 4 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.750133\n",
      "Model 4 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.636488\n",
      "Model 4 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.365106\n",
      "Model 4 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.360313\n",
      "Model 4 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.339763\n",
      "Model 4 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.191367\n",
      "Model 4 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.235346\n",
      "Model 4 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.210301\n",
      "\n",
      "Model 1 Test set: Average loss: 1.0261, Accuracy: 6915/10000 (69%)\n",
      "\n",
      "\n",
      "Model 2 Test set: Average loss: 0.9037, Accuracy: 7153/10000 (72%)\n",
      "\n",
      "\n",
      "Model 3 Test set: Average loss: 1.0121, Accuracy: 6565/10000 (66%)\n",
      "\n",
      "\n",
      "Model 4 Test set: Average loss: 1.0766, Accuracy: 6238/10000 (62%)\n",
      "\n",
      "Accuracy of Ensemble of networks with 4 models: 7679/10000 (77%)\n",
      "\n",
      "Number of parameters: 3210\n",
      "Model 1 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.386576\n",
      "Model 1 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.122695\n",
      "Model 1 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.920559\n",
      "Model 1 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.860352\n",
      "Model 1 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.793113\n",
      "Model 1 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.534647\n",
      "Model 1 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.792705\n",
      "Model 1 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.617881\n",
      "Model 1 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.417356\n",
      "Model 1 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.334034\n",
      "Number of parameters: 3210\n",
      "Model 2 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.421528\n",
      "Model 2 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.855565\n",
      "Model 2 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.620099\n",
      "Model 2 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.562243\n",
      "Model 2 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.612088\n",
      "Model 2 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.412502\n",
      "Model 2 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.370805\n",
      "Model 2 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.639166\n",
      "Model 2 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.368885\n",
      "Model 2 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.430554\n",
      "Number of parameters: 3210\n",
      "Model 3 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.324691\n",
      "Model 3 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.857005\n",
      "Model 3 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.844701\n",
      "Model 3 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.619932\n",
      "Model 3 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.638621\n",
      "Model 3 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.575437\n",
      "Model 3 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.492101\n",
      "Model 3 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.441728\n",
      "Model 3 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.438002\n",
      "Model 3 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.256849\n",
      "Number of parameters: 3210\n",
      "Model 4 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.504478\n",
      "Model 4 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.049671\n",
      "Model 4 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.803586\n",
      "Model 4 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.519959\n",
      "Model 4 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.589078\n",
      "Model 4 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.365571\n",
      "Model 4 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.388142\n",
      "Model 4 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.309811\n",
      "Model 4 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.204387\n",
      "Model 4 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.240795\n",
      "Number of parameters: 3210\n",
      "Model 5 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.477621\n",
      "Model 5 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.147068\n",
      "Model 5 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.863732\n",
      "Model 5 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.880030\n",
      "Model 5 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.663526\n",
      "Model 5 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.294808\n",
      "Model 5 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.324753\n",
      "Model 5 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.177151\n",
      "Model 5 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.316044\n",
      "Model 5 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.222743\n",
      "Number of parameters: 3210\n",
      "Model 6 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.467247\n",
      "Model 6 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.066753\n",
      "Model 6 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.070381\n",
      "Model 6 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.746764\n",
      "Model 6 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.889410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 6 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.712603\n",
      "Model 6 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.439458\n",
      "Model 6 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.538598\n",
      "Model 6 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.387978\n",
      "Model 6 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.349545\n",
      "Number of parameters: 3210\n",
      "Model 7 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.322679\n",
      "Model 7 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.878052\n",
      "Model 7 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.702726\n",
      "Model 7 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.705114\n",
      "Model 7 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.417118\n",
      "Model 7 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.282214\n",
      "Model 7 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.150392\n",
      "Model 7 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.887207\n",
      "Model 7 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.151562\n",
      "Model 7 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.175729\n",
      "Number of parameters: 3210\n",
      "Model 8 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.287571\n",
      "Model 8 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.057301\n",
      "Model 8 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.954951\n",
      "Model 8 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.900306\n",
      "Model 8 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.744037\n",
      "Model 8 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.818803\n",
      "Model 8 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.454564\n",
      "Model 8 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.481841\n",
      "Model 8 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.281747\n",
      "Model 8 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.373571\n",
      "\n",
      "Model 1 Test set: Average loss: 1.2969, Accuracy: 5570/10000 (56%)\n",
      "\n",
      "\n",
      "Model 2 Test set: Average loss: 1.2896, Accuracy: 4770/10000 (48%)\n",
      "\n",
      "\n",
      "Model 3 Test set: Average loss: 1.2340, Accuracy: 6098/10000 (61%)\n",
      "\n",
      "\n",
      "Model 4 Test set: Average loss: 1.2159, Accuracy: 5595/10000 (56%)\n",
      "\n",
      "\n",
      "Model 5 Test set: Average loss: 1.1545, Accuracy: 5751/10000 (58%)\n",
      "\n",
      "\n",
      "Model 6 Test set: Average loss: 1.4071, Accuracy: 5198/10000 (52%)\n",
      "\n",
      "\n",
      "Model 7 Test set: Average loss: 0.9224, Accuracy: 7209/10000 (72%)\n",
      "\n",
      "\n",
      "Model 8 Test set: Average loss: 1.1956, Accuracy: 5547/10000 (55%)\n",
      "\n",
      "Accuracy of Ensemble of networks with 8 models: 7787/10000 (78%)\n",
      "\n",
      "Number of parameters: 3210\n",
      "Model 1 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.571549\n",
      "Model 1 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.192707\n",
      "Model 1 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.841849\n",
      "Model 1 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.815032\n",
      "Model 1 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.789388\n",
      "Model 1 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.580805\n",
      "Model 1 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.277800\n",
      "Model 1 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.045285\n",
      "Model 1 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.147646\n",
      "Model 1 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.268119\n",
      "Number of parameters: 3210\n",
      "Model 2 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.537418\n",
      "Model 2 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.880787\n",
      "Model 2 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.828753\n",
      "Model 2 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.598645\n",
      "Model 2 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.445913\n",
      "Model 2 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.711037\n",
      "Model 2 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.183893\n",
      "Model 2 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.251193\n",
      "Model 2 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.038540\n",
      "Model 2 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.108223\n",
      "Number of parameters: 3210\n",
      "Model 3 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.425963\n",
      "Model 3 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.164652\n",
      "Model 3 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.758697\n",
      "Model 3 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.657982\n",
      "Model 3 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.663719\n",
      "Model 3 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.648139\n",
      "Model 3 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.398019\n",
      "Model 3 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.416158\n",
      "Model 3 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.380695\n",
      "Model 3 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.523722\n",
      "Number of parameters: 3210\n",
      "Model 4 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.451673\n",
      "Model 4 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.919415\n",
      "Model 4 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.633349\n",
      "Model 4 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.320934\n",
      "Model 4 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.185543\n",
      "Model 4 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.237916\n",
      "Model 4 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.223793\n",
      "Model 4 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.188127\n",
      "Model 4 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.861410\n",
      "Model 4 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.888697\n",
      "Number of parameters: 3210\n",
      "Model 5 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.451076\n",
      "Model 5 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.974244\n",
      "Model 5 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.564720\n",
      "Model 5 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.217496\n",
      "Model 5 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.322520\n",
      "Model 5 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.976257\n",
      "Model 5 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.989309\n",
      "Model 5 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.058971\n",
      "Model 5 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.784331\n",
      "Model 5 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.801011\n",
      "Number of parameters: 3210\n",
      "Model 6 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.342946\n",
      "Model 6 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.071613\n",
      "Model 6 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.743483\n",
      "Model 6 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.655345\n",
      "Model 6 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.753144\n",
      "Model 6 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.647570\n",
      "Model 6 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.377184\n",
      "Model 6 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.429394\n",
      "Model 6 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.539615\n",
      "Model 6 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.520672\n",
      "Number of parameters: 3210\n",
      "Model 7 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.463844\n",
      "Model 7 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.774820\n",
      "Model 7 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.510295\n",
      "Model 7 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.483985\n",
      "Model 7 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.904947\n",
      "Model 7 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.927896\n",
      "Model 7 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.756211\n",
      "Model 7 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.745107\n",
      "Model 7 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.756668\n",
      "Model 7 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.744672\n",
      "Number of parameters: 3210\n",
      "Model 8 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.566732\n",
      "Model 8 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.826411\n",
      "Model 8 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.506481\n",
      "Model 8 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.331104\n",
      "Model 8 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.292838\n",
      "Model 8 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.100243\n",
      "Model 8 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.155757\n",
      "Model 8 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.035172\n",
      "Model 8 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.112293\n",
      "Model 8 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.854415\n",
      "Number of parameters: 3210\n",
      "Model 9 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.467614\n",
      "Model 9 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.787443\n",
      "Model 9 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.543616\n",
      "Model 9 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.394397\n",
      "Model 9 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.381704\n",
      "Model 9 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.210205\n",
      "Model 9 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.103654\n",
      "Model 9 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.124006\n",
      "Model 9 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.053916\n",
      "Model 9 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.976863\n",
      "Number of parameters: 3210\n",
      "Model 10 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.369605\n",
      "Model 10 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.593246\n",
      "Model 10 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.487240\n",
      "Model 10 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.159747\n",
      "Model 10 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.133707\n",
      "Model 10 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.188240\n",
      "Model 10 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.152607\n",
      "Model 10 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.219172\n",
      "Model 10 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.698089\n",
      "Model 10 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.823484\n",
      "Number of parameters: 3210\n",
      "Model 11 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.322907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 11 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.833506\n",
      "Model 11 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.820405\n",
      "Model 11 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.415996\n",
      "Model 11 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.402968\n",
      "Model 11 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.127608\n",
      "Model 11 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.305788\n",
      "Model 11 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.989194\n",
      "Model 11 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.051177\n",
      "Model 11 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.798511\n",
      "Number of parameters: 3210\n",
      "Model 12 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.516184\n",
      "Model 12 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.996755\n",
      "Model 12 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.768652\n",
      "Model 12 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.729856\n",
      "Model 12 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.500293\n",
      "Model 12 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.390318\n",
      "Model 12 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.137016\n",
      "Model 12 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.378708\n",
      "Model 12 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.365578\n",
      "Model 12 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.197205\n",
      "Number of parameters: 3210\n",
      "Model 13 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.316314\n",
      "Model 13 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.142057\n",
      "Model 13 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.890754\n",
      "Model 13 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.833870\n",
      "Model 13 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.819790\n",
      "Model 13 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.475633\n",
      "Model 13 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.610118\n",
      "Model 13 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.324314\n",
      "Model 13 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.338922\n",
      "Model 13 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.321408\n",
      "Number of parameters: 3210\n",
      "Model 14 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.346093\n",
      "Model 14 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.998679\n",
      "Model 14 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.662956\n",
      "Model 14 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.623820\n",
      "Model 14 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.331824\n",
      "Model 14 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.315667\n",
      "Model 14 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.272626\n",
      "Model 14 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.960893\n",
      "Model 14 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.205092\n",
      "Model 14 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.035794\n",
      "Number of parameters: 3210\n",
      "Model 15 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.774801\n",
      "Model 15 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.999075\n",
      "Model 15 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.857799\n",
      "Model 15 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.495169\n",
      "Model 15 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.700593\n",
      "Model 15 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.329677\n",
      "Model 15 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.594864\n",
      "Model 15 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.526689\n",
      "Model 15 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.553651\n",
      "Model 15 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.468413\n",
      "Number of parameters: 3210\n",
      "Model 16 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.475341\n",
      "Model 16 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.846818\n",
      "Model 16 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.779258\n",
      "Model 16 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.326505\n",
      "Model 16 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.292158\n",
      "Model 16 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.401592\n",
      "Model 16 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.177313\n",
      "Model 16 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.059934\n",
      "Model 16 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.142830\n",
      "Model 16 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.111693\n",
      "\n",
      "Model 1 Test set: Average loss: 1.0407, Accuracy: 6353/10000 (64%)\n",
      "\n",
      "\n",
      "Model 2 Test set: Average loss: 1.0540, Accuracy: 6341/10000 (63%)\n",
      "\n",
      "\n",
      "Model 3 Test set: Average loss: 1.4062, Accuracy: 5048/10000 (50%)\n",
      "\n",
      "\n",
      "Model 4 Test set: Average loss: 0.9518, Accuracy: 7064/10000 (71%)\n",
      "\n",
      "\n",
      "Model 5 Test set: Average loss: 0.9014, Accuracy: 7264/10000 (73%)\n",
      "\n",
      "\n",
      "Model 6 Test set: Average loss: 1.4087, Accuracy: 4779/10000 (48%)\n",
      "\n",
      "\n",
      "Model 7 Test set: Average loss: 0.7070, Accuracy: 7862/10000 (79%)\n",
      "\n",
      "\n",
      "Model 8 Test set: Average loss: 1.0121, Accuracy: 6827/10000 (68%)\n",
      "\n",
      "\n",
      "Model 9 Test set: Average loss: 0.8866, Accuracy: 7107/10000 (71%)\n",
      "\n",
      "\n",
      "Model 10 Test set: Average loss: 0.8930, Accuracy: 7162/10000 (72%)\n",
      "\n",
      "\n",
      "Model 11 Test set: Average loss: 0.8775, Accuracy: 7270/10000 (73%)\n",
      "\n",
      "\n",
      "Model 12 Test set: Average loss: 1.0474, Accuracy: 6362/10000 (64%)\n",
      "\n",
      "\n",
      "Model 13 Test set: Average loss: 1.2606, Accuracy: 5247/10000 (52%)\n",
      "\n",
      "\n",
      "Model 14 Test set: Average loss: 1.0413, Accuracy: 6844/10000 (68%)\n",
      "\n",
      "\n",
      "Model 15 Test set: Average loss: 1.4011, Accuracy: 4755/10000 (48%)\n",
      "\n",
      "\n",
      "Model 16 Test set: Average loss: 1.0144, Accuracy: 6892/10000 (69%)\n",
      "\n",
      "Accuracy of Ensemble of networks with 16 models: 8571/10000 (86%)\n",
      "\n",
      "Number of parameters: 3210\n",
      "Model 1 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.453719\n",
      "Model 1 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.969194\n",
      "Model 1 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.857662\n",
      "Model 1 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.706038\n",
      "Model 1 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.325502\n",
      "Model 1 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.532467\n",
      "Model 1 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.301177\n",
      "Model 1 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.222242\n",
      "Model 1 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.919127\n",
      "Model 1 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.225856\n",
      "Number of parameters: 3210\n",
      "Model 2 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.318933\n",
      "Model 2 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.893698\n",
      "Model 2 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.788046\n",
      "Model 2 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.628870\n",
      "Model 2 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.646403\n",
      "Model 2 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.436767\n",
      "Model 2 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.956413\n",
      "Model 2 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.937730\n",
      "Model 2 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.878726\n",
      "Model 2 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.127457\n",
      "Number of parameters: 3210\n",
      "Model 3 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.642636\n",
      "Model 3 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.076784\n",
      "Model 3 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.615394\n",
      "Model 3 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.169001\n",
      "Model 3 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.152909\n",
      "Model 3 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.937713\n",
      "Model 3 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.928224\n",
      "Model 3 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.978842\n",
      "Model 3 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.624262\n",
      "Model 3 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.849909\n",
      "Number of parameters: 3210\n",
      "Model 4 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.351740\n",
      "Model 4 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.043829\n",
      "Model 4 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.759448\n",
      "Model 4 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.650171\n",
      "Model 4 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.641446\n",
      "Model 4 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.361664\n",
      "Model 4 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.628618\n",
      "Model 4 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.332659\n",
      "Model 4 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.475855\n",
      "Model 4 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.055355\n",
      "Number of parameters: 3210\n",
      "Model 5 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.317343\n",
      "Model 5 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.826605\n",
      "Model 5 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.682221\n",
      "Model 5 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.655195\n",
      "Model 5 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.600282\n",
      "Model 5 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.722092\n",
      "Model 5 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.342150\n",
      "Model 5 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.215408\n",
      "Model 5 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.063785\n",
      "Model 5 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.477075\n",
      "Number of parameters: 3210\n",
      "Model 6 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.297915\n",
      "Model 6 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.002981\n",
      "Model 6 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.552033\n",
      "Model 6 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.639487\n",
      "Model 6 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.272352\n",
      "Model 6 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.374300\n",
      "Model 6 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.160981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 6 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.981510\n",
      "Model 6 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.275145\n",
      "Model 6 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.097485\n",
      "Number of parameters: 3210\n",
      "Model 7 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.376100\n",
      "Model 7 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.857804\n",
      "Model 7 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.646925\n",
      "Model 7 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.409196\n",
      "Model 7 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.325343\n",
      "Model 7 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.238941\n",
      "Model 7 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.478067\n",
      "Model 7 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.270684\n",
      "Model 7 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.017907\n",
      "Model 7 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.061114\n",
      "Number of parameters: 3210\n",
      "Model 8 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.386905\n",
      "Model 8 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.003167\n",
      "Model 8 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.665320\n",
      "Model 8 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.653489\n",
      "Model 8 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.223620\n",
      "Model 8 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.101188\n",
      "Model 8 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.145059\n",
      "Model 8 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.866407\n",
      "Model 8 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.904173\n",
      "Model 8 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.122977\n",
      "Number of parameters: 3210\n",
      "Model 9 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.358205\n",
      "Model 9 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.742178\n",
      "Model 9 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.783059\n",
      "Model 9 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.438745\n",
      "Model 9 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.393620\n",
      "Model 9 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.260192\n",
      "Model 9 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.085677\n",
      "Model 9 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.173451\n",
      "Model 9 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.044935\n",
      "Model 9 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.851742\n",
      "Number of parameters: 3210\n",
      "Model 10 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.639686\n",
      "Model 10 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.022928\n",
      "Model 10 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.778406\n",
      "Model 10 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.272222\n",
      "Model 10 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.246365\n",
      "Model 10 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.321171\n",
      "Model 10 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.040564\n",
      "Model 10 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.227764\n",
      "Model 10 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.923130\n",
      "Model 10 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.282262\n",
      "Number of parameters: 3210\n",
      "Model 11 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.323872\n",
      "Model 11 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.132800\n",
      "Model 11 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.953295\n",
      "Model 11 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.630438\n",
      "Model 11 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.652911\n",
      "Model 11 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.279048\n",
      "Model 11 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.367718\n",
      "Model 11 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.363111\n",
      "Model 11 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.419520\n",
      "Model 11 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.260324\n",
      "Number of parameters: 3210\n",
      "Model 12 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.903415\n",
      "Model 12 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.987830\n",
      "Model 12 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.818455\n",
      "Model 12 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.712011\n",
      "Model 12 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.639575\n",
      "Model 12 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.653933\n",
      "Model 12 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.548031\n",
      "Model 12 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.486574\n",
      "Model 12 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.271545\n",
      "Model 12 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.363917\n",
      "Number of parameters: 3210\n",
      "Model 13 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.762779\n",
      "Model 13 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.058470\n",
      "Model 13 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.107638\n",
      "Model 13 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.815647\n",
      "Model 13 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.689761\n",
      "Model 13 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.742951\n",
      "Model 13 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.697195\n",
      "Model 13 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.616822\n",
      "Model 13 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.664772\n",
      "Model 13 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.616600\n",
      "Number of parameters: 3210\n",
      "Model 14 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.542396\n",
      "Model 14 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.875841\n",
      "Model 14 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.694440\n",
      "Model 14 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.435377\n",
      "Model 14 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.164393\n",
      "Model 14 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.333031\n",
      "Model 14 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.401677\n",
      "Model 14 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.049482\n",
      "Model 14 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.049999\n",
      "Model 14 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.284788\n",
      "Number of parameters: 3210\n",
      "Model 15 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.430104\n",
      "Model 15 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.981888\n",
      "Model 15 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.675211\n",
      "Model 15 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.631101\n",
      "Model 15 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.659401\n",
      "Model 15 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.424283\n",
      "Model 15 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.363195\n",
      "Model 15 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.551145\n",
      "Model 15 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.536462\n",
      "Model 15 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.215969\n",
      "Number of parameters: 3210\n",
      "Model 16 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.457082\n",
      "Model 16 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.133333\n",
      "Model 16 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.629620\n",
      "Model 16 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.425974\n",
      "Model 16 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.417866\n",
      "Model 16 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.380581\n",
      "Model 16 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.384687\n",
      "Model 16 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.183742\n",
      "Model 16 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.057963\n",
      "Model 16 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.357381\n",
      "Number of parameters: 3210\n",
      "Model 17 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.308943\n",
      "Model 17 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.229931\n",
      "Model 17 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.113262\n",
      "Model 17 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 2.059575\n",
      "Model 17 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.905372\n",
      "Model 17 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.627050\n",
      "Model 17 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.782998\n",
      "Model 17 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.694508\n",
      "Model 17 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.560834\n",
      "Model 17 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.506477\n",
      "Number of parameters: 3210\n",
      "Model 18 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.341411\n",
      "Model 18 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.937206\n",
      "Model 18 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.439080\n",
      "Model 18 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.374400\n",
      "Model 18 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.412423\n",
      "Model 18 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.276448\n",
      "Model 18 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.067894\n",
      "Model 18 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.068007\n",
      "Model 18 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.797574\n",
      "Model 18 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.035352\n",
      "Number of parameters: 3210\n",
      "Model 19 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.391078\n",
      "Model 19 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.060066\n",
      "Model 19 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.824821\n",
      "Model 19 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.734398\n",
      "Model 19 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.525193\n",
      "Model 19 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.448056\n",
      "Model 19 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.545385\n",
      "Model 19 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.315623\n",
      "Model 19 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.346259\n",
      "Model 19 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.385539\n",
      "Number of parameters: 3210\n",
      "Model 20 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.224010\n",
      "Model 20 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.733635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 20 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.673146\n",
      "Model 20 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.477693\n",
      "Model 20 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.131025\n",
      "Model 20 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.166045\n",
      "Model 20 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.234090\n",
      "Model 20 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.915536\n",
      "Model 20 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.020958\n",
      "Model 20 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.891926\n",
      "Number of parameters: 3210\n",
      "Model 21 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.543577\n",
      "Model 21 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.219797\n",
      "Model 21 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.072996\n",
      "Model 21 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.909333\n",
      "Model 21 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.821849\n",
      "Model 21 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.912856\n",
      "Model 21 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.917869\n",
      "Model 21 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.772717\n",
      "Model 21 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.560258\n",
      "Model 21 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.510614\n",
      "Number of parameters: 3210\n",
      "Model 22 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.810393\n",
      "Model 22 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.864881\n",
      "Model 22 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.883344\n",
      "Model 22 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.463282\n",
      "Model 22 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.592581\n",
      "Model 22 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.251498\n",
      "Model 22 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.314895\n",
      "Model 22 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.094305\n",
      "Model 22 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.970270\n",
      "Model 22 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.140639\n",
      "Number of parameters: 3210\n",
      "Model 23 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.804847\n",
      "Model 23 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.574673\n",
      "Model 23 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.373091\n",
      "Model 23 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.905789\n",
      "Model 23 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.845635\n",
      "Model 23 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.900012\n",
      "Model 23 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.792386\n",
      "Model 23 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.702290\n",
      "Model 23 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.550269\n",
      "Model 23 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.746973\n",
      "Number of parameters: 3210\n",
      "Model 24 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.330890\n",
      "Model 24 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.004978\n",
      "Model 24 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.749336\n",
      "Model 24 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.586403\n",
      "Model 24 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.361902\n",
      "Model 24 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.312010\n",
      "Model 24 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.060666\n",
      "Model 24 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.098820\n",
      "Model 24 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.179029\n",
      "Model 24 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.000110\n",
      "Number of parameters: 3210\n",
      "Model 25 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.306610\n",
      "Model 25 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.688709\n",
      "Model 25 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.590001\n",
      "Model 25 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.335294\n",
      "Model 25 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.445496\n",
      "Model 25 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.238234\n",
      "Model 25 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.050002\n",
      "Model 25 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.164701\n",
      "Model 25 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.939582\n",
      "Model 25 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.158208\n",
      "Number of parameters: 3210\n",
      "Model 26 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.331525\n",
      "Model 26 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.920416\n",
      "Model 26 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.590543\n",
      "Model 26 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.347291\n",
      "Model 26 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.241582\n",
      "Model 26 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.209825\n",
      "Model 26 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.412292\n",
      "Model 26 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.305609\n",
      "Model 26 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.120909\n",
      "Model 26 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.083924\n",
      "Number of parameters: 3210\n",
      "Model 27 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.759356\n",
      "Model 27 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.902903\n",
      "Model 27 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.732748\n",
      "Model 27 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.750133\n",
      "Model 27 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.588992\n",
      "Model 27 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.212749\n",
      "Model 27 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.162398\n",
      "Model 27 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.123383\n",
      "Model 27 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.105953\n",
      "Model 27 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.823137\n",
      "Number of parameters: 3210\n",
      "Model 28 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.328680\n",
      "Model 28 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.011634\n",
      "Model 28 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.569052\n",
      "Model 28 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.286917\n",
      "Model 28 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.203028\n",
      "Model 28 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.156162\n",
      "Model 28 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.029987\n",
      "Model 28 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.822863\n",
      "Model 28 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.262359\n",
      "Model 28 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.670814\n",
      "Number of parameters: 3210\n",
      "Model 29 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.434676\n",
      "Model 29 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.099479\n",
      "Model 29 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.845005\n",
      "Model 29 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.758304\n",
      "Model 29 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.781891\n",
      "Model 29 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.594567\n",
      "Model 29 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.690332\n",
      "Model 29 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.640972\n",
      "Model 29 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.761174\n",
      "Model 29 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.727121\n",
      "Number of parameters: 3210\n",
      "Model 30 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.284400\n",
      "Model 30 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.871173\n",
      "Model 30 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.567229\n",
      "Model 30 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.558510\n",
      "Model 30 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.246470\n",
      "Model 30 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.256861\n",
      "Model 30 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.179897\n",
      "Model 30 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.095824\n",
      "Model 30 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.020478\n",
      "Model 30 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.012692\n",
      "Number of parameters: 3210\n",
      "Model 31 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.258106\n",
      "Model 31 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.966219\n",
      "Model 31 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.466202\n",
      "Model 31 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.568265\n",
      "Model 31 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.313242\n",
      "Model 31 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.365943\n",
      "Model 31 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.264582\n",
      "Model 31 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.246097\n",
      "Model 31 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.013177\n",
      "Model 31 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.104256\n",
      "Number of parameters: 3210\n",
      "Model 32 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.326496\n",
      "Model 32 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.828505\n",
      "Model 32 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.663088\n",
      "Model 32 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.749310\n",
      "Model 32 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.654109\n",
      "Model 32 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.530437\n",
      "Model 32 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.282429\n",
      "Model 32 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.290225\n",
      "Model 32 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.045822\n",
      "Model 32 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.997214\n",
      "\n",
      "Model 1 Test set: Average loss: 1.1422, Accuracy: 5730/10000 (57%)\n",
      "\n",
      "\n",
      "Model 2 Test set: Average loss: 0.8983, Accuracy: 7326/10000 (73%)\n",
      "\n",
      "\n",
      "Model 3 Test set: Average loss: 0.7771, Accuracy: 7573/10000 (76%)\n",
      "\n",
      "\n",
      "Model 4 Test set: Average loss: 1.2155, Accuracy: 6544/10000 (65%)\n",
      "\n",
      "\n",
      "Model 5 Test set: Average loss: 1.2758, Accuracy: 5604/10000 (56%)\n",
      "\n",
      "\n",
      "Model 6 Test set: Average loss: 1.0804, Accuracy: 6367/10000 (64%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 7 Test set: Average loss: 1.0301, Accuracy: 6429/10000 (64%)\n",
      "\n",
      "\n",
      "Model 8 Test set: Average loss: 0.9349, Accuracy: 7210/10000 (72%)\n",
      "\n",
      "\n",
      "Model 9 Test set: Average loss: 0.9007, Accuracy: 7054/10000 (71%)\n",
      "\n",
      "\n",
      "Model 10 Test set: Average loss: 1.0667, Accuracy: 6340/10000 (63%)\n",
      "\n",
      "\n",
      "Model 11 Test set: Average loss: 1.2761, Accuracy: 5144/10000 (51%)\n",
      "\n",
      "\n",
      "Model 12 Test set: Average loss: 1.2409, Accuracy: 5632/10000 (56%)\n",
      "\n",
      "\n",
      "Model 13 Test set: Average loss: 1.4567, Accuracy: 4669/10000 (47%)\n",
      "\n",
      "\n",
      "Model 14 Test set: Average loss: 1.1282, Accuracy: 6038/10000 (60%)\n",
      "\n",
      "\n",
      "Model 15 Test set: Average loss: 1.3329, Accuracy: 4687/10000 (47%)\n",
      "\n",
      "\n",
      "Model 16 Test set: Average loss: 1.0349, Accuracy: 6475/10000 (65%)\n",
      "\n",
      "\n",
      "Model 17 Test set: Average loss: 1.5455, Accuracy: 4466/10000 (45%)\n",
      "\n",
      "\n",
      "Model 18 Test set: Average loss: 0.9159, Accuracy: 7064/10000 (71%)\n",
      "\n",
      "\n",
      "Model 19 Test set: Average loss: 1.1967, Accuracy: 6685/10000 (67%)\n",
      "\n",
      "\n",
      "Model 20 Test set: Average loss: 0.8398, Accuracy: 7969/10000 (80%)\n",
      "\n",
      "\n",
      "Model 21 Test set: Average loss: 1.4920, Accuracy: 4322/10000 (43%)\n",
      "\n",
      "\n",
      "Model 22 Test set: Average loss: 0.9427, Accuracy: 6976/10000 (70%)\n",
      "\n",
      "\n",
      "Model 23 Test set: Average loss: 0.7329, Accuracy: 7708/10000 (77%)\n",
      "\n",
      "\n",
      "Model 24 Test set: Average loss: 0.9498, Accuracy: 6626/10000 (66%)\n",
      "\n",
      "\n",
      "Model 25 Test set: Average loss: 1.0466, Accuracy: 6549/10000 (65%)\n",
      "\n",
      "\n",
      "Model 26 Test set: Average loss: 1.0462, Accuracy: 7040/10000 (70%)\n",
      "\n",
      "\n",
      "Model 27 Test set: Average loss: 0.9879, Accuracy: 6774/10000 (68%)\n",
      "\n",
      "\n",
      "Model 28 Test set: Average loss: 0.8454, Accuracy: 7540/10000 (75%)\n",
      "\n",
      "\n",
      "Model 29 Test set: Average loss: 1.6292, Accuracy: 4432/10000 (44%)\n",
      "\n",
      "\n",
      "Model 30 Test set: Average loss: 1.0498, Accuracy: 6825/10000 (68%)\n",
      "\n",
      "\n",
      "Model 31 Test set: Average loss: 1.1581, Accuracy: 5998/10000 (60%)\n",
      "\n",
      "\n",
      "Model 32 Test set: Average loss: 1.1327, Accuracy: 6084/10000 (61%)\n",
      "\n",
      "Accuracy of Ensemble of networks with 32 models: 8814/10000 (88%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc5d68c7400>]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "# load the MNIST dataset\n",
    "input_size  = 28*28   # images are 28x28 pixels\n",
    "output_size = 10      # there are 10 classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True)\n",
    "\n",
    "#Create paths to store trained models\n",
    "TRAINING_PATH = \"./training/\"\n",
    "try:\n",
    "    os.mkdir(TRAINING_PATH)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# define function for weight initialization\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# define the multi-layer perceptron model\n",
    "class FC2Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FC2Layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, output_size), \n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)\n",
    "    \n",
    "\n",
    "# define the training loop\n",
    "def train(epoch, model, k):\n",
    "    for i in range(k):\n",
    "        #initiailize weight\n",
    "        np.random.seed(i + np.random.randint(1001)) \n",
    "        model.apply(weights_init)\n",
    "        # initialize the optimizer\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=moment)\n",
    "        print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass through the model\n",
    "            output = model(data)\n",
    "            # forward pass through the cross-entropy loss function\n",
    "            loss = F.nll_loss(output, target)\n",
    "            # backward pass through the cross-entropy loss function and the model\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Model {} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(i+1,\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "        #Save inidvidual models of an emsemble\n",
    "        torch.save(model.state_dict(), TRAINING_PATH + 'model{}_state_dict'.format(i+1) + \".pt\")\n",
    "\n",
    "# define the testing loop\n",
    "ensemble_accuracy_container = []\n",
    "def test(model, k):\n",
    "    for i in range(k):\n",
    "        #load individual models of an ensemble \n",
    "        model.load_state_dict(torch.load(TRAINING_PATH + 'model{}_state_dict'.format(i+1) + \".pt\"))\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss \n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print('\\nModel {} Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(i+1,\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            accuracy))\n",
    "    \n",
    "    #ensemble accuracy\n",
    "    ensemble_correct = 0\n",
    "    for data, target in test_loader:\n",
    "        ensemble_predict = np.zeros((1000,1)) #placeholder\n",
    "        for i in range(k):\n",
    "            model.load_state_dict(torch.load(TRAINING_PATH + 'model{}_state_dict'.format(i+1) + \".pt\"))\n",
    "            model.eval()\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1] \n",
    "            np_pred = pred.numpy()\n",
    "            ensemble_predict = np.hstack((ensemble_predict, np_pred))\n",
    "            \n",
    "        #MAJORITY VOTING OF DIFFERENT MODELS ON EACH BATCH\n",
    "        ensemble_predict = ensemble_predict[:,1:]\n",
    "        #VOTE:\n",
    "        ensemble_predict = torch.from_numpy(stats.mode(ensemble_predict, axis = 1)[0])\n",
    "        ensemble_correct += ensemble_predict.eq(target.data.view_as(ensemble_predict)).cpu().sum().item()\n",
    "    ensemble_accuracy = 100. * ensemble_correct / len(test_loader.dataset)\n",
    "    print(\"Accuracy of Ensemble of networks with {} models: {}/{} ({:.0f}%)\\n\".format(k, ensemble_correct, \n",
    "            len(test_loader.dataset),\n",
    "            ensemble_accuracy))\n",
    "    ensemble_accuracy_container.append(ensemble_accuracy)\n",
    "\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np\n",
    "    \n",
    "# build the model and execute the training and testing\n",
    "\n",
    "# initialize some hyper-paramters \n",
    "n_hidden = 4 # number of hidden layers\n",
    "learning_rate = 0.01\n",
    "moment = 0.5\n",
    "nepochs = 1\n",
    "\n",
    "k_list = [1,2,4,8,16,32]\n",
    "\n",
    "# build the actual model\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "\n",
    "# train the model for one epoch\n",
    "for k in k_list:\n",
    "    for epoch in range(0, nepochs):\n",
    "        train(epoch, model_fnn, k)\n",
    "        test(model_fnn,k)\n",
    "        \n",
    "#PLOT:\n",
    "plt.style.use('ggplot')\n",
    "plt.xlabel('Number of Models in Emsemble')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(k_list, ensemble_accuracy_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b: Bagged networks  \n",
    "Now we will extend the above code to create a bag of `k` single hidden layer MLP models with each with `4` hidden units. We compare the performance of `5` bags, with sizes $k = \\{2, 4, 8, 16, 32\\}$ respectively. We train each model inside each bag for `1` epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3210\n",
      "Training... k = 2\n",
      "Model 1 Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.363631\n",
      "Model 1 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.036662\n",
      "Model 1 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.915916\n",
      "Model 1 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.320144\n",
      "Model 1 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.128820\n",
      "Model 1 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.077625\n",
      "Model 1 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.931791\n",
      "Model 1 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.060292\n",
      "Model 1 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.004222\n",
      "Model 1 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.826369\n",
      "Number of parameters: 3210\n",
      "Training... k = 2\n",
      "Model 2 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.940817\n",
      "Model 2 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.700183\n",
      "Model 2 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.572123\n",
      "Model 2 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.637110\n",
      "Model 2 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.744002\n",
      "Model 2 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.713339\n",
      "Model 2 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.135702\n",
      "Model 2 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.539756\n",
      "Model 2 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.802089\n",
      "Model 2 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.683091\n",
      "\n",
      "Model 1 Test set: Average loss: 0.8245, Accuracy: 7484/10000 (75%)\n",
      "\n",
      "\n",
      "Model 2 Test set: Average loss: 0.6703, Accuracy: 7947/10000 (79%)\n",
      "\n",
      "Accuracy of Ensemble of networks with 2 models: 7676/10000 (77%)\n",
      "\n",
      "Number of parameters: 3210\n",
      "Training... k = 4\n",
      "Model 1 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.581463\n",
      "Model 1 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.983264\n",
      "Model 1 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.894343\n",
      "Model 1 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.597099\n",
      "Model 1 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.337157\n",
      "Model 1 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.905277\n",
      "Model 1 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.506734\n",
      "Model 1 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.507522\n",
      "Model 1 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.573728\n",
      "Model 1 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.568422\n",
      "Number of parameters: 3210\n",
      "Training... k = 4\n",
      "Model 2 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.635883\n",
      "Model 2 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.821514\n",
      "Model 2 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.822158\n",
      "Model 2 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.531753\n",
      "Model 2 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.465792\n",
      "Model 2 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.531009\n",
      "Model 2 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.458511\n",
      "Model 2 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.575984\n",
      "Model 2 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.412625\n",
      "Model 2 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.526089\n",
      "Number of parameters: 3210\n",
      "Training... k = 4\n",
      "Model 3 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.708729\n",
      "Model 3 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.580020\n",
      "Model 3 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.400554\n",
      "Model 3 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.794416\n",
      "Model 3 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.524478\n",
      "Model 3 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.638714\n",
      "Model 3 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.637770\n",
      "Model 3 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.926081\n",
      "Model 3 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.862751\n",
      "Model 3 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.614463\n",
      "Number of parameters: 3210\n",
      "Training... k = 4\n",
      "Model 4 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.630290\n",
      "Model 4 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.720606\n",
      "Model 4 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.858016\n",
      "Model 4 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.647560\n",
      "Model 4 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.532597\n",
      "Model 4 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.918177\n",
      "Model 4 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.645046\n",
      "Model 4 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.315318\n",
      "Model 4 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.699136\n",
      "Model 4 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.419060\n",
      "\n",
      "Model 1 Test set: Average loss: 0.6493, Accuracy: 8001/10000 (80%)\n",
      "\n",
      "\n",
      "Model 2 Test set: Average loss: 0.5976, Accuracy: 8251/10000 (83%)\n",
      "\n",
      "\n",
      "Model 3 Test set: Average loss: 0.6181, Accuracy: 8158/10000 (82%)\n",
      "\n",
      "\n",
      "Model 4 Test set: Average loss: 0.5884, Accuracy: 8265/10000 (83%)\n",
      "\n",
      "Accuracy of Ensemble of networks with 4 models: 8303/10000 (83%)\n",
      "\n",
      "Number of parameters: 3210\n",
      "Training... k = 8\n",
      "Model 1 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.533637\n",
      "Model 1 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.507951\n",
      "Model 1 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.494044\n",
      "Model 1 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.781986\n",
      "Model 1 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.790092\n",
      "Model 1 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.825599\n",
      "Model 1 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.666256\n",
      "Model 1 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.600487\n",
      "Model 1 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.478896\n",
      "Model 1 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.784529\n",
      "Number of parameters: 3210\n",
      "Training... k = 8\n",
      "Model 2 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.558887\n",
      "Model 2 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.544408\n",
      "Model 2 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.399645\n",
      "Model 2 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.402215\n",
      "Model 2 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.887886\n",
      "Model 2 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.578966\n",
      "Model 2 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.404443\n",
      "Model 2 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.675563\n",
      "Model 2 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.739043\n",
      "Model 2 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.581920\n",
      "Number of parameters: 3210\n",
      "Training... k = 8\n",
      "Model 3 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.533879\n",
      "Model 3 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.529939\n",
      "Model 3 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.561217\n",
      "Model 3 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.460970\n",
      "Model 3 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.493574\n",
      "Model 3 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.281593\n",
      "Model 3 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.609472\n",
      "Model 3 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.591820\n",
      "Model 3 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.666420\n",
      "Model 3 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.681973\n",
      "Number of parameters: 3210\n",
      "Training... k = 8\n",
      "Model 4 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.580403\n",
      "Model 4 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.464796\n",
      "Model 4 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.532537\n",
      "Model 4 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.491537\n",
      "Model 4 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.558327\n",
      "Model 4 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.422597\n",
      "Model 4 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.511198\n",
      "Model 4 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.666657\n",
      "Model 4 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.607778\n",
      "Model 4 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.711485\n",
      "Number of parameters: 3210\n",
      "Training... k = 8\n",
      "Model 5 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.496330\n",
      "Model 5 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.484051\n",
      "Model 5 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.816183\n",
      "Model 5 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.576577\n",
      "Model 5 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.519050\n",
      "Model 5 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.563603\n",
      "Model 5 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.375266\n",
      "Model 5 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.457794\n",
      "Model 5 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.846290\n",
      "Model 5 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.417710\n",
      "Number of parameters: 3210\n",
      "Training... k = 8\n",
      "Model 6 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.692562\n",
      "Model 6 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.753948\n",
      "Model 6 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.445790\n",
      "Model 6 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.650818\n",
      "Model 6 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.386564\n",
      "Model 6 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.664235\n",
      "Model 6 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.424639\n",
      "Model 6 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.578709\n",
      "Model 6 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.518865\n",
      "Model 6 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.813835\n",
      "Number of parameters: 3210\n",
      "Training... k = 8\n",
      "Model 7 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.391418\n",
      "Model 7 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.523928\n",
      "Model 7 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.741497\n",
      "Model 7 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.507078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 7 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.434602\n",
      "Model 7 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.353059\n",
      "Model 7 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.419944\n",
      "Model 7 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.367488\n",
      "Model 7 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.423437\n",
      "Model 7 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.430878\n",
      "Number of parameters: 3210\n",
      "Training... k = 8\n",
      "Model 8 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.462661\n",
      "Model 8 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.806819\n",
      "Model 8 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.560043\n",
      "Model 8 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.625165\n",
      "Model 8 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.453577\n",
      "Model 8 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.430007\n",
      "Model 8 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.539553\n",
      "Model 8 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.504122\n",
      "Model 8 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.372405\n",
      "Model 8 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.345251\n",
      "\n",
      "Model 1 Test set: Average loss: 0.5820, Accuracy: 8282/10000 (83%)\n",
      "\n",
      "\n",
      "Model 2 Test set: Average loss: 0.5718, Accuracy: 8267/10000 (83%)\n",
      "\n",
      "\n",
      "Model 3 Test set: Average loss: 0.6156, Accuracy: 8101/10000 (81%)\n",
      "\n",
      "\n",
      "Model 4 Test set: Average loss: 0.5572, Accuracy: 8342/10000 (83%)\n",
      "\n",
      "\n",
      "Model 5 Test set: Average loss: 0.5549, Accuracy: 8381/10000 (84%)\n",
      "\n",
      "\n",
      "Model 6 Test set: Average loss: 0.5426, Accuracy: 8342/10000 (83%)\n",
      "\n",
      "\n",
      "Model 7 Test set: Average loss: 0.5376, Accuracy: 8390/10000 (84%)\n",
      "\n",
      "\n",
      "Model 8 Test set: Average loss: 0.5821, Accuracy: 8197/10000 (82%)\n",
      "\n",
      "Accuracy of Ensemble of networks with 8 models: 8383/10000 (84%)\n",
      "\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 1 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.452631\n",
      "Model 1 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.536183\n",
      "Model 1 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.297659\n",
      "Model 1 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.337969\n",
      "Model 1 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.890010\n",
      "Model 1 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.579234\n",
      "Model 1 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.445816\n",
      "Model 1 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.480044\n",
      "Model 1 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.319476\n",
      "Model 1 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.705628\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 2 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.377952\n",
      "Model 2 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.340225\n",
      "Model 2 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.562585\n",
      "Model 2 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.716864\n",
      "Model 2 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.388063\n",
      "Model 2 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.424699\n",
      "Model 2 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.391297\n",
      "Model 2 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.707339\n",
      "Model 2 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.780244\n",
      "Model 2 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.388811\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 3 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.386030\n",
      "Model 3 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.574547\n",
      "Model 3 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.557812\n",
      "Model 3 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.329401\n",
      "Model 3 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.594984\n",
      "Model 3 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.431211\n",
      "Model 3 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.713448\n",
      "Model 3 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.704174\n",
      "Model 3 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.462058\n",
      "Model 3 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.580103\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 4 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.391809\n",
      "Model 4 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.496536\n",
      "Model 4 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.358523\n",
      "Model 4 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.647455\n",
      "Model 4 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.541025\n",
      "Model 4 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.503550\n",
      "Model 4 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.528778\n",
      "Model 4 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.359770\n",
      "Model 4 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.339463\n",
      "Model 4 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.656013\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 5 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.713079\n",
      "Model 5 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.443314\n",
      "Model 5 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.419224\n",
      "Model 5 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.458128\n",
      "Model 5 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.494838\n",
      "Model 5 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.540260\n",
      "Model 5 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.401628\n",
      "Model 5 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.873689\n",
      "Model 5 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.544592\n",
      "Model 5 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.381641\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 6 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.361296\n",
      "Model 6 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.533365\n",
      "Model 6 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.673387\n",
      "Model 6 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.602375\n",
      "Model 6 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.328658\n",
      "Model 6 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.316150\n",
      "Model 6 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.537695\n",
      "Model 6 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.573315\n",
      "Model 6 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.328360\n",
      "Model 6 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.399723\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 7 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.234670\n",
      "Model 7 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.417652\n",
      "Model 7 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.707314\n",
      "Model 7 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.443708\n",
      "Model 7 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.513055\n",
      "Model 7 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.289397\n",
      "Model 7 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.501364\n",
      "Model 7 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.672495\n",
      "Model 7 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.628616\n",
      "Model 7 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.436436\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 8 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.377489\n",
      "Model 8 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.526354\n",
      "Model 8 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.496737\n",
      "Model 8 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.269304\n",
      "Model 8 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.444460\n",
      "Model 8 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.682234\n",
      "Model 8 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.368852\n",
      "Model 8 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.427871\n",
      "Model 8 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.255961\n",
      "Model 8 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.648729\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 9 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.438845\n",
      "Model 9 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.391422\n",
      "Model 9 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.546480\n",
      "Model 9 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.451510\n",
      "Model 9 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.472185\n",
      "Model 9 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.775816\n",
      "Model 9 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.624894\n",
      "Model 9 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.352543\n",
      "Model 9 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.614358\n",
      "Model 9 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.598960\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 10 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.650078\n",
      "Model 10 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.404354\n",
      "Model 10 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.411731\n",
      "Model 10 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.406720\n",
      "Model 10 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.370193\n",
      "Model 10 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.876053\n",
      "Model 10 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.426918\n",
      "Model 10 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.740110\n",
      "Model 10 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.491404\n",
      "Model 10 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.616988\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 11 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.419445\n",
      "Model 11 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.493598\n",
      "Model 11 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.459888\n",
      "Model 11 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.519190\n",
      "Model 11 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.480324\n",
      "Model 11 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.609012\n",
      "Model 11 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.335094\n",
      "Model 11 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.404903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 11 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.424994\n",
      "Model 11 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.377914\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 12 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.559369\n",
      "Model 12 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.739208\n",
      "Model 12 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.768585\n",
      "Model 12 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.466089\n",
      "Model 12 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.656711\n",
      "Model 12 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.387812\n",
      "Model 12 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.595684\n",
      "Model 12 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.395302\n",
      "Model 12 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.631165\n",
      "Model 12 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.209905\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 13 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.387400\n",
      "Model 13 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.454389\n",
      "Model 13 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.454598\n",
      "Model 13 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.312653\n",
      "Model 13 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.242478\n",
      "Model 13 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.368849\n",
      "Model 13 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.335322\n",
      "Model 13 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.642679\n",
      "Model 13 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.311016\n",
      "Model 13 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.515876\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 14 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.507946\n",
      "Model 14 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.406255\n",
      "Model 14 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.701718\n",
      "Model 14 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.405139\n",
      "Model 14 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.372654\n",
      "Model 14 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.767189\n",
      "Model 14 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.488101\n",
      "Model 14 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.502768\n",
      "Model 14 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.415416\n",
      "Model 14 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.213894\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 15 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.366594\n",
      "Model 15 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.265809\n",
      "Model 15 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.399004\n",
      "Model 15 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.659606\n",
      "Model 15 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.465272\n",
      "Model 15 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.384407\n",
      "Model 15 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.390401\n",
      "Model 15 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.417550\n",
      "Model 15 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.342801\n",
      "Model 15 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.669077\n",
      "Number of parameters: 3210\n",
      "Training... k = 16\n",
      "Model 16 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.546545\n",
      "Model 16 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.269424\n",
      "Model 16 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.744455\n",
      "Model 16 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.592436\n",
      "Model 16 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.523933\n",
      "Model 16 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.352001\n",
      "Model 16 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.434535\n",
      "Model 16 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.645936\n",
      "Model 16 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.709373\n",
      "Model 16 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.425758\n",
      "\n",
      "Model 1 Test set: Average loss: 0.5228, Accuracy: 8462/10000 (85%)\n",
      "\n",
      "\n",
      "Model 2 Test set: Average loss: 0.5330, Accuracy: 8457/10000 (85%)\n",
      "\n",
      "\n",
      "Model 3 Test set: Average loss: 0.5158, Accuracy: 8495/10000 (85%)\n",
      "\n",
      "\n",
      "Model 4 Test set: Average loss: 0.5105, Accuracy: 8521/10000 (85%)\n",
      "\n",
      "\n",
      "Model 5 Test set: Average loss: 0.5097, Accuracy: 8508/10000 (85%)\n",
      "\n",
      "\n",
      "Model 6 Test set: Average loss: 0.5079, Accuracy: 8520/10000 (85%)\n",
      "\n",
      "\n",
      "Model 7 Test set: Average loss: 0.5246, Accuracy: 8503/10000 (85%)\n",
      "\n",
      "\n",
      "Model 8 Test set: Average loss: 0.4968, Accuracy: 8601/10000 (86%)\n",
      "\n",
      "\n",
      "Model 9 Test set: Average loss: 0.5190, Accuracy: 8500/10000 (85%)\n",
      "\n",
      "\n",
      "Model 10 Test set: Average loss: 0.4926, Accuracy: 8582/10000 (86%)\n",
      "\n",
      "\n",
      "Model 11 Test set: Average loss: 0.4939, Accuracy: 8580/10000 (86%)\n",
      "\n",
      "\n",
      "Model 12 Test set: Average loss: 0.5008, Accuracy: 8583/10000 (86%)\n",
      "\n",
      "\n",
      "Model 13 Test set: Average loss: 0.5212, Accuracy: 8468/10000 (85%)\n",
      "\n",
      "\n",
      "Model 14 Test set: Average loss: 0.5032, Accuracy: 8533/10000 (85%)\n",
      "\n",
      "\n",
      "Model 15 Test set: Average loss: 0.4841, Accuracy: 8603/10000 (86%)\n",
      "\n",
      "\n",
      "Model 16 Test set: Average loss: 0.4936, Accuracy: 8545/10000 (85%)\n",
      "\n",
      "Accuracy of Ensemble of networks with 16 models: 8627/10000 (86%)\n",
      "\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 1 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.406774\n",
      "Model 1 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.342669\n",
      "Model 1 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.429196\n",
      "Model 1 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.436760\n",
      "Model 1 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.293197\n",
      "Model 1 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.566797\n",
      "Model 1 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.882743\n",
      "Model 1 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.460860\n",
      "Model 1 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.671363\n",
      "Model 1 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.294998\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 2 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.472728\n",
      "Model 2 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.740857\n",
      "Model 2 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.515392\n",
      "Model 2 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.476930\n",
      "Model 2 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.498285\n",
      "Model 2 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.606301\n",
      "Model 2 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.303636\n",
      "Model 2 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.432268\n",
      "Model 2 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.383224\n",
      "Model 2 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.252958\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 3 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.350658\n",
      "Model 3 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.451618\n",
      "Model 3 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.354136\n",
      "Model 3 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.314048\n",
      "Model 3 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.377678\n",
      "Model 3 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.570431\n",
      "Model 3 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.408028\n",
      "Model 3 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.444700\n",
      "Model 3 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.410105\n",
      "Model 3 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.549444\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 4 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.565205\n",
      "Model 4 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.275434\n",
      "Model 4 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.557019\n",
      "Model 4 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.561474\n",
      "Model 4 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.333210\n",
      "Model 4 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.741739\n",
      "Model 4 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.412466\n",
      "Model 4 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.508187\n",
      "Model 4 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.330687\n",
      "Model 4 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.377075\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 5 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.425017\n",
      "Model 5 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.493173\n",
      "Model 5 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.657810\n",
      "Model 5 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.506633\n",
      "Model 5 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.499460\n",
      "Model 5 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.498778\n",
      "Model 5 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.422964\n",
      "Model 5 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.451422\n",
      "Model 5 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.323674\n",
      "Model 5 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.530686\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 6 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.421114\n",
      "Model 6 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.585607\n",
      "Model 6 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.424384\n",
      "Model 6 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.501455\n",
      "Model 6 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.598650\n",
      "Model 6 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.455800\n",
      "Model 6 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.390421\n",
      "Model 6 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.835348\n",
      "Model 6 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.613612\n",
      "Model 6 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.723300\n",
      "Number of parameters: 3210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... k = 32\n",
      "Model 7 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.406375\n",
      "Model 7 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.379700\n",
      "Model 7 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.459383\n",
      "Model 7 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.026927\n",
      "Model 7 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.615563\n",
      "Model 7 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.497583\n",
      "Model 7 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.821582\n",
      "Model 7 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.278808\n",
      "Model 7 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.379409\n",
      "Model 7 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.495151\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 8 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.419569\n",
      "Model 8 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.504191\n",
      "Model 8 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.702198\n",
      "Model 8 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.482227\n",
      "Model 8 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.269241\n",
      "Model 8 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.336456\n",
      "Model 8 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.357043\n",
      "Model 8 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.285273\n",
      "Model 8 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.467774\n",
      "Model 8 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.484204\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 9 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.474275\n",
      "Model 9 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.445738\n",
      "Model 9 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.647633\n",
      "Model 9 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.440108\n",
      "Model 9 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.436364\n",
      "Model 9 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.227065\n",
      "Model 9 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.379226\n",
      "Model 9 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.491329\n",
      "Model 9 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.301157\n",
      "Model 9 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.630671\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 10 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.229618\n",
      "Model 10 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.744830\n",
      "Model 10 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.709657\n",
      "Model 10 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.378441\n",
      "Model 10 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.401435\n",
      "Model 10 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.355290\n",
      "Model 10 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.294884\n",
      "Model 10 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.482016\n",
      "Model 10 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.639488\n",
      "Model 10 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.379902\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 11 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.549631\n",
      "Model 11 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.644500\n",
      "Model 11 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.263040\n",
      "Model 11 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.364698\n",
      "Model 11 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.384439\n",
      "Model 11 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.424757\n",
      "Model 11 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.246332\n",
      "Model 11 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.495191\n",
      "Model 11 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.540592\n",
      "Model 11 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.609318\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 12 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.500211\n",
      "Model 12 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.707736\n",
      "Model 12 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.399830\n",
      "Model 12 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.397954\n",
      "Model 12 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.390013\n",
      "Model 12 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.737061\n",
      "Model 12 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.506618\n",
      "Model 12 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.688140\n",
      "Model 12 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.326228\n",
      "Model 12 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.424640\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 13 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.570454\n",
      "Model 13 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.726547\n",
      "Model 13 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.605302\n",
      "Model 13 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.489910\n",
      "Model 13 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.466199\n",
      "Model 13 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.703794\n",
      "Model 13 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.618392\n",
      "Model 13 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.477103\n",
      "Model 13 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.437071\n",
      "Model 13 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.399023\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 14 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.346849\n",
      "Model 14 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.213620\n",
      "Model 14 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.312766\n",
      "Model 14 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.636430\n",
      "Model 14 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.546399\n",
      "Model 14 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.575437\n",
      "Model 14 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.380731\n",
      "Model 14 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.362442\n",
      "Model 14 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.579816\n",
      "Model 14 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.311102\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 15 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.534993\n",
      "Model 15 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.627556\n",
      "Model 15 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.240795\n",
      "Model 15 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.385932\n",
      "Model 15 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.521131\n",
      "Model 15 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.617482\n",
      "Model 15 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.710363\n",
      "Model 15 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.646656\n",
      "Model 15 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.451917\n",
      "Model 15 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.307481\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 16 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.345171\n",
      "Model 16 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.330690\n",
      "Model 16 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.483957\n",
      "Model 16 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.425797\n",
      "Model 16 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.260466\n",
      "Model 16 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.391062\n",
      "Model 16 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.320130\n",
      "Model 16 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.312564\n",
      "Model 16 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.475655\n",
      "Model 16 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.235713\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 17 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.545390\n",
      "Model 17 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.508584\n",
      "Model 17 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.869285\n",
      "Model 17 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.318861\n",
      "Model 17 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.305565\n",
      "Model 17 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.448934\n",
      "Model 17 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.797325\n",
      "Model 17 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.495520\n",
      "Model 17 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.504004\n",
      "Model 17 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.494498\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 18 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.299650\n",
      "Model 18 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.652863\n",
      "Model 18 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.333097\n",
      "Model 18 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.569029\n",
      "Model 18 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.241822\n",
      "Model 18 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.325029\n",
      "Model 18 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.560343\n",
      "Model 18 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.437344\n",
      "Model 18 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.277686\n",
      "Model 18 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.373680\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 19 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.818673\n",
      "Model 19 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.367629\n",
      "Model 19 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.323334\n",
      "Model 19 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.679019\n",
      "Model 19 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.406830\n",
      "Model 19 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.411355\n",
      "Model 19 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.337985\n",
      "Model 19 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.398827\n",
      "Model 19 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.409007\n",
      "Model 19 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.402000\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 20 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.358780\n",
      "Model 20 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.491666\n",
      "Model 20 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.389395\n",
      "Model 20 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.416419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 20 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.462440\n",
      "Model 20 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.207593\n",
      "Model 20 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.548398\n",
      "Model 20 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.521300\n",
      "Model 20 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.515409\n",
      "Model 20 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.361516\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 21 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.579222\n",
      "Model 21 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.363510\n",
      "Model 21 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.531901\n",
      "Model 21 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.325303\n",
      "Model 21 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.525645\n",
      "Model 21 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.492713\n",
      "Model 21 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.508574\n",
      "Model 21 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.492218\n",
      "Model 21 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.343364\n",
      "Model 21 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.343133\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 22 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.765802\n",
      "Model 22 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.593798\n",
      "Model 22 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.594006\n",
      "Model 22 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.340915\n",
      "Model 22 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.336578\n",
      "Model 22 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.355382\n",
      "Model 22 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.394431\n",
      "Model 22 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.311090\n",
      "Model 22 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.408482\n",
      "Model 22 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.395920\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 23 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.273632\n",
      "Model 23 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.611555\n",
      "Model 23 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.597167\n",
      "Model 23 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.528076\n",
      "Model 23 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.305137\n",
      "Model 23 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.333094\n",
      "Model 23 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.438394\n",
      "Model 23 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.367793\n",
      "Model 23 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.305769\n",
      "Model 23 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.493989\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 24 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.622811\n",
      "Model 24 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.475547\n",
      "Model 24 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.294196\n",
      "Model 24 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.393972\n",
      "Model 24 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.238546\n",
      "Model 24 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.556610\n",
      "Model 24 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.449610\n",
      "Model 24 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.516763\n",
      "Model 24 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.237747\n",
      "Model 24 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.243297\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 25 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.368644\n",
      "Model 25 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.372779\n",
      "Model 25 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.318432\n",
      "Model 25 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.219695\n",
      "Model 25 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.725867\n",
      "Model 25 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.450798\n",
      "Model 25 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.308398\n",
      "Model 25 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.514601\n",
      "Model 25 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.295700\n",
      "Model 25 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.666734\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 26 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.422209\n",
      "Model 26 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.522391\n",
      "Model 26 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.406940\n",
      "Model 26 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.322256\n",
      "Model 26 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.592218\n",
      "Model 26 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.531653\n",
      "Model 26 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.463466\n",
      "Model 26 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.409195\n",
      "Model 26 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.696621\n",
      "Model 26 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.667924\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 27 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.527060\n",
      "Model 27 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.433378\n",
      "Model 27 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.554023\n",
      "Model 27 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.613958\n",
      "Model 27 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.513961\n",
      "Model 27 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.618052\n",
      "Model 27 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.262667\n",
      "Model 27 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.398345\n",
      "Model 27 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.923568\n",
      "Model 27 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.306913\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 28 Train Epoch: 0 [0/60000 (0%)]\tLoss: 1.065488\n",
      "Model 28 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.261757\n",
      "Model 28 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.435942\n",
      "Model 28 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.505784\n",
      "Model 28 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.372944\n",
      "Model 28 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.382215\n",
      "Model 28 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.545194\n",
      "Model 28 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.451700\n",
      "Model 28 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.367343\n",
      "Model 28 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.409396\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 29 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.412664\n",
      "Model 29 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.681586\n",
      "Model 29 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.301004\n",
      "Model 29 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.292072\n",
      "Model 29 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.200536\n",
      "Model 29 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.455812\n",
      "Model 29 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.297403\n",
      "Model 29 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.342163\n",
      "Model 29 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.431902\n",
      "Model 29 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.399183\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 30 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.636327\n",
      "Model 30 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.563774\n",
      "Model 30 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.650035\n",
      "Model 30 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.517315\n",
      "Model 30 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.460920\n",
      "Model 30 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.613667\n",
      "Model 30 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.644420\n",
      "Model 30 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.403164\n",
      "Model 30 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.615959\n",
      "Model 30 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.352095\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 31 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.646177\n",
      "Model 31 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.421503\n",
      "Model 31 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.492175\n",
      "Model 31 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.465427\n",
      "Model 31 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.321412\n",
      "Model 31 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.461411\n",
      "Model 31 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.301185\n",
      "Model 31 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.351714\n",
      "Model 31 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.511520\n",
      "Model 31 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.439029\n",
      "Number of parameters: 3210\n",
      "Training... k = 32\n",
      "Model 32 Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.276040\n",
      "Model 32 Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.403617\n",
      "Model 32 Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.429392\n",
      "Model 32 Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.281785\n",
      "Model 32 Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.361130\n",
      "Model 32 Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.619979\n",
      "Model 32 Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.550742\n",
      "Model 32 Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.888357\n",
      "Model 32 Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.418905\n",
      "Model 32 Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.681177\n",
      "\n",
      "Model 1 Test set: Average loss: 0.4919, Accuracy: 8586/10000 (86%)\n",
      "\n",
      "\n",
      "Model 2 Test set: Average loss: 0.4732, Accuracy: 8630/10000 (86%)\n",
      "\n",
      "\n",
      "Model 3 Test set: Average loss: 0.4910, Accuracy: 8623/10000 (86%)\n",
      "\n",
      "\n",
      "Model 4 Test set: Average loss: 0.4804, Accuracy: 8631/10000 (86%)\n",
      "\n",
      "\n",
      "Model 5 Test set: Average loss: 0.4828, Accuracy: 8620/10000 (86%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 6 Test set: Average loss: 0.4865, Accuracy: 8621/10000 (86%)\n",
      "\n",
      "\n",
      "Model 7 Test set: Average loss: 0.4822, Accuracy: 8641/10000 (86%)\n",
      "\n",
      "\n",
      "Model 8 Test set: Average loss: 0.4757, Accuracy: 8655/10000 (87%)\n",
      "\n",
      "\n",
      "Model 9 Test set: Average loss: 0.4804, Accuracy: 8627/10000 (86%)\n",
      "\n",
      "\n",
      "Model 10 Test set: Average loss: 0.4928, Accuracy: 8596/10000 (86%)\n",
      "\n",
      "\n",
      "Model 11 Test set: Average loss: 0.4920, Accuracy: 8618/10000 (86%)\n",
      "\n",
      "\n",
      "Model 12 Test set: Average loss: 0.4807, Accuracy: 8659/10000 (87%)\n",
      "\n",
      "\n",
      "Model 13 Test set: Average loss: 0.4816, Accuracy: 8651/10000 (87%)\n",
      "\n",
      "\n",
      "Model 14 Test set: Average loss: 0.4835, Accuracy: 8605/10000 (86%)\n",
      "\n",
      "\n",
      "Model 15 Test set: Average loss: 0.4786, Accuracy: 8625/10000 (86%)\n",
      "\n",
      "\n",
      "Model 16 Test set: Average loss: 0.4724, Accuracy: 8670/10000 (87%)\n",
      "\n",
      "\n",
      "Model 17 Test set: Average loss: 0.4789, Accuracy: 8613/10000 (86%)\n",
      "\n",
      "\n",
      "Model 18 Test set: Average loss: 0.4854, Accuracy: 8602/10000 (86%)\n",
      "\n",
      "\n",
      "Model 19 Test set: Average loss: 0.4777, Accuracy: 8653/10000 (87%)\n",
      "\n",
      "\n",
      "Model 20 Test set: Average loss: 0.4777, Accuracy: 8647/10000 (86%)\n",
      "\n",
      "\n",
      "Model 21 Test set: Average loss: 0.4742, Accuracy: 8662/10000 (87%)\n",
      "\n",
      "\n",
      "Model 22 Test set: Average loss: 0.4793, Accuracy: 8648/10000 (86%)\n",
      "\n",
      "\n",
      "Model 23 Test set: Average loss: 0.4705, Accuracy: 8673/10000 (87%)\n",
      "\n",
      "\n",
      "Model 24 Test set: Average loss: 0.4664, Accuracy: 8673/10000 (87%)\n",
      "\n",
      "\n",
      "Model 25 Test set: Average loss: 0.4719, Accuracy: 8660/10000 (87%)\n",
      "\n",
      "\n",
      "Model 26 Test set: Average loss: 0.4800, Accuracy: 8628/10000 (86%)\n",
      "\n",
      "\n",
      "Model 27 Test set: Average loss: 0.4879, Accuracy: 8607/10000 (86%)\n",
      "\n",
      "\n",
      "Model 28 Test set: Average loss: 0.4749, Accuracy: 8655/10000 (87%)\n",
      "\n",
      "\n",
      "Model 29 Test set: Average loss: 0.4695, Accuracy: 8699/10000 (87%)\n",
      "\n",
      "\n",
      "Model 30 Test set: Average loss: 0.4781, Accuracy: 8626/10000 (86%)\n",
      "\n",
      "\n",
      "Model 31 Test set: Average loss: 0.4749, Accuracy: 8643/10000 (86%)\n",
      "\n",
      "\n",
      "Model 32 Test set: Average loss: 0.4799, Accuracy: 8639/10000 (86%)\n",
      "\n",
      "Accuracy of Ensemble of networks with 32 models: 8711/10000 (87%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc5a7a3c160>]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEJCAYAAAByupuRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAm8klEQVR4nO3deXxU1f3/8dedJBOEkI0JxET2RQQlalmCilSNS7G0/VqMbd3Qn8IXUL/FPiy1i/qt+Cg+2HyoWGsR3KqCX+uGRW2ghCpawqZRkE0lyCCQnazDZM7vj4FAJMgAydyZue/nX8mdzNzP4ep5zz333HssY4xBREQcyWV3ASIiYh+FgIiIgykEREQcTCEgIuJgCgEREQdTCIiIOFi83QWcDK/Xa3cJJ8zj8VBaWmp3GacsVtoBakukipW2RFo7srKyWt2uMwEREQdTCIiIOJhCQETEwRQCIiIOphAQEXEwhYCIiIMpBEREHCwq7xMQEYllxn8AqiqhqhyqKjCV5VBVjnVhHlZGZpvuSyEgIhIm5oAPKoMd+5Gd+7d/pqb66DdbLqy+A0EhICISWUxjQ7ADr6zAVFVAVRn7fQ0EdnsxVeWHO/66mqPf7HJBchqkpIGnG1bfs4I/p6ZhpXSB1IOvdU7Fiotr89oVAiIirTDGQEN969/UKyuCnfuh3+vrjnp/XXz84c49MxvrzHMOdu7pWCnpzR09SSlYLvsuzyoERMRRjDFQV3u4cz/ym3pVBaayrPlnGhuO/oAEd7ADT0mD7J5Yg847unNPScfTqzdlZWXhb+AJUgiISEwwxkDN/lY792939BzwHf0BiR2aO3erZ7/DHf23Onc6dsKyrOPWE8rfRAKFgIhENBMIBC+UttKht+jcqyvA7z/6A07r2NyBW30GHh5jT0nHSk0/3NF36Bj+xkUAhYCI2MIEmqC66ogLqsEOvbqxnqY93sOd+/5KaGo6+gM6Jh3uwLtlHx5jT+mC1fxzOlZih7C3LZooBESkTRm/P/itvKoCqsoxlRWtX1ytrgITOOr9Dcmp0Dkl2Lln9zz6W/uhjj/BHf7GxSCFgIiExBw4EOzcD3bkpqq1nw/OcTem5ZstK9ixp6RBahesHn1bH29PSSUj8/SIWowl1ikERBzO+BoPTns8Tudeu//oN7tckJwa7MC7dMXqc2brnXty+8xxl1OnEBCJUaahvrlzN83z21vp3Otrj35zXDykpAY78K5ZWAMGtzIskw6dk7Fc6tyjmUJAJIoYY4I3JlVVQGXZwbtTK9jfWEfgG2/Lzr2x/ugPiE9o/qZOVg+ss3IOD9EccTGVTp1tvYFJwkchIBIBgjcw1UBl8JEDwYupBy+oVh785n7oDlXf0XPc6xI7BIdlUtOxevQ5/E09NT3YuR/8OdQ57uIcCgGRdmQCgeBY+pGd+ZFDMkd+c/cfOPoDOpx2uDPvPeCYnbvnjO5RcXeqRB6FgMhJMIEm2F999Df1ym917tUVx5jj3ulwZ95/UOude0oaVofTQqpH3+7lZCkERL6D2fklZv2HLTv3qnKoroTA0XPcSep8sANPx8o84/ANS0deTE1Jw3Inhr0tIq1RCIi0wlRXYl5/AfP+P4MbkpIPf1M/o9e3vrUfvNCanIaVkGBr3SInSiEgcgTjP4BZtgTz9iLwNWJd9iOsH16H1SnJ7tJE2kXYQmDJkiUsX74cy7Lo3r07kydPxu12s3TpUt555x3i4uI4//zzueGGG8JVkkgzYwx8vJrAKwtg7244Zyiu/FuDQzoiMSwsIVBeXs7SpUuZO3cubrebOXPmsGrVKjweD2vWrGHWrFkkJCRQVVUVjnJEWjC7dhBYNB82fQynd8f1Pw9gnX2+3WWJhEXYzgQCgQA+n4+4uDh8Ph9paWm89957/PjHPybh4DhqSkpKuMoRIVBdSeBvf8YUvgundcT6+QSsi6/CitcoqTiHZcy3n/TUPv7xj3/w0ksv4Xa7ycnJ4a677uKee+5h2LBhbNiwgYSEBG688Ub69et31HsLCgooKCgAYMaMGfhauVkm0sXHx+Nv7VnnUSYW2mH8fuqWvkrtogWY+jpO+8F/kXTdbbg6J9td2kmLheNySKy0JdLa4Xa3/tTVsIRATU0Ns2fPZurUqXTs2JE5c+aQm5vLG2+8weDBg7nlllvYvn07c+fO5fHHHz/unGev19veJbc5j8cTE09GjPZ2mOI1BBY/Dd/swn3eCPw/uRErq4fdZZ2yaD8uR4qVtkRaO7KyslrdHpbz3uLiYrp27UpycvCb1ogRI9iyZQvp6emMGDECy7Lo168fLpeL/fv3N/+dSFsxu3cGO/9P10G3bFx3/oHUS67SXbbieGEJAY/Hw9atW2lsbMTtdlNcXEzfvn3p0aMHn376KYMHD8br9eL3++ncuXM4ShKHMLU1mLdewvzrbUg8DevaW7EuvRorPkF32YoQphDo378/ubm5TJs2jbi4OHr16kVeXh6WZfHEE0/wq1/9ivj4eKZMmaL/MaVNmKYmzMp3MG+8CHW1WKOuwPrJ9VidNflA5EhhmwaRn59Pfn7+UdvvuuuucJUgDmE2riew6GnwlsDAIbiu+39YZ/S2uyyRiKS5cBIzzB5v8Gavj1dDRiauyb+Fc0fo7FLkOygEJOqZulrM24swy5ZAQgLWT28OPu5Bz/EROS6FgEQtE2jCvP9PzOt/g5pqrAvzsH5yQ/ChbiISEoWARCWzuZjAy/Ph6y+h36Dgox569rW7LJGooxCQqGL2fUPg/xbCug+hS1dcE38N37tQ4/4iJ0khIFHBNNRh/vEK5p9vQFx8cNjn8h9rcRaRU6QQkIhmAgHMh8sxrz0PVRVYIy/BuuYmrNQudpcmEhMUAhKxzNaNwUc879gGfQfimvK74GLrItJmFAIScUzZPsyrz2CK/g1pHqzbfoU1/GKN+4u0A4WARAzT2IB551XMu68BYP3wZ1hXXYOV2MHmykRil0JAbGcCAczqQsyrz0FlWfBb/zU3Y3XJsLs0kZinEBBbmS82E3j5r/DlFujZD9fEX2P1O8vuskQcQyEgtjAVZZi/P4v5aAWkpGPd8j9YuZdguVx2lybiKAoBCSvT2Ih57zXMO69CIIA15lqsH4zD6nCa3aWJOJJCQMLCGINZ8z7m/56B8n3wvQtw/XQ8Vkam3aWJOJpCQNqd2bEt+JyfbRuhe29ct07FOvNsu8sSERQC0o5MVQXmtecwq5ZDUjLWTXdgXXgZlivO7tJE5CCFgLQ5c8CHKXgT8/Yr4D+AdcVPsMbkY3XsZHdpIvItCgFpM8YYWP8hgVcWQukeOHcErmtvweqaZXdpInIMCgFpE6bkCwKLn4bNxZDdE9fdD2KdlWN3WSJyHAoBOSWmuhLz+guY9/8JnZKwrv9vrFFXYsVp3F8kGigE5KQY/wHMsiWYtxeBrxHrsrHBZ/10SrK7NBE5AQoBOSHGGMyG/xB4ZQHs3Q3nDMV17a1Yp59hd2kichIUAhIys2sHlY8/SODjIsg8A9f/3I919vfsLktEToFCQI7L1FRj3nwRU/gOB07rhPWz27FG/wArXv/5iEQ7/V8sx2T8fsyKf2Deegka6rFGX4Vn/J2U+w7YXZqItBGFgLTKFK8NTvn85msYdC6u/NuwsnvgSk6B0lK7yxORNqIQkBbM7q+Dnf+na6Hr6bju+D0MGaalHUVilEJAADC1NZi3XsKs+Ae4E7GuvQXr0h9ixSfYXZqItKOwhcCSJUtYvnw5lmXRvXt3Jk+ejNvtBuDNN9/khRdeYP78+SQnJ4erJAFMUxNm5buYN/8GtTVYo67A+vH1WMmpdpcmImEQlhAoLy9n6dKlzJ07F7fbzZw5c1i1ahXf//73KS0tpbi4GI/HE45S5Ahm44bg0M+uHXDmObiuuw2re2+7yxKRMArbmUAgEMDn8xEXF4fP5yMtLQ2AZ599luuvv56ZM2eGqxTHM3u8wZu9Pl4Nnm64Jv0GzhupcX8RBwpLCKSnpzN27FgmTZqE2+0mJyeHnJwc1qxZQ3p6Or169QpHGY5n6moxby/GLHsL4hOwrrkJK+9HWAluu0sTEZuEJQRqamooKipi3rx5dOzYkTlz5lBYWMi7777L73//++O+v6CggIKCAgBmzJgRlUNH8fHxttVtmpqoX7aE2hefwlRX0uGSMSRdP5G49BOvx852tDW1JTLFSluipR2WMca0904+/PBDNmzYwKRJkwAoLCxkxYoV7Ny5k8TERADKyspIS0vjT3/6E6mpqd/5eV6vt71LbnMej4dSG+bXm82fElj0V9j5JfQ7C9fPbsfq2e+kP8+udrQHtSUyxUpbIq0dWVmtr+sRljMBj8fD1q1baWxsxO12U1xczPDhw7n//vub/2bKlCn86U9/0uygNmAO+DAbVgcf77xxPaRnYE24B2voRRr3F5EWwhIC/fv3Jzc3l2nTphEXF0evXr3Iy8sLx64dwxgDX23DrCrArF4JdbWQ5sH6yQ1Yl/8Yy51od4kiEoHCNjsoPz+f/Pz8Y74+b968cJUSU0xVBeajf2E+WAa7d0KCG+u8kVgXXgoDh2hRdxH5TrpjOAqZAwfgkyICHxTAZ+sgEIC+A7FunIw1dJQWdBeRkCkEooQxBkq+wKxahvlPIdTuh9R0rCv/C+uCy7AytaiLiJw4hUCEM9WVmP8UYj4oCN7ZG5+AdV4u1gWXwqBzNdwjIqdEIRCBjP8AFK8lsGoZFK+BpiboPSC4iPuwi7WOr4i0GYVABDE7vwwO93y0AmqqISUteEfvBZdhZfWwuzwRiUEKAZuZ/dWY1QeHe3Z+CfHxkDMc1wWXweDzseI03CMi7UchYAPj98Nn64LDPR8XQZMfevbD+vkErOEXYyXphjkRCQ+FQBiZXTsOD/dUV0LnFKxLr8a64FKsM/QIZxEJP4VAOzO1+zGrV1L2n0IC2z+HuDgYMiw43HP297DidQhExD4h9UA7duygZ8+e7V1LzDBNTbBxPeaDZZiP/wN+P/Tqj3XdbVgjRmN1TrG7RBERIMQQ+OMf/0h6ejqjRo1i1KhRzQvCSEtm985gx//RCqgqh6RkrNE/wLrgMrqcPzyinigoIgIhhsBTTz3FunXr+Pe//80rr7zCmWeeycUXX8yIESOaHwXtVKauBrP635hVy+DLLeBywTlDg8M9Q4ZqoXYRiWghhUBcXBzDhg1j2LBh1NXV8eGHH/Lmm28yf/58hg8fTl5eHgMHDmzvWiOKaWzEvDAPs+YD8B+A7J5Y196KlTsaK1lnSiISHU7oqmRDQwOrV69m1apVlJWVccEFF+DxeHjsscc477zzuO2229qrzohj1q/CfLQCa9QVWKOvgh599ax+EYk6IYXAunXrWLlyJevXr2fgwIFceumlTJs2Dbc7uDbtVVddxaRJkxwVAuwqgbh4rF/8t2b4iEjUCqn3+tvf/sbo0aO5+eabW70onJSUxPjx49u6tohmvCWQma0AEJGoFlIPNnv27OP+zWWXXXbKxUSVXTuw+pxpdxUiIqfEFcofzZo1i02bNrXYtmnTppDCIRaZhnoo2wt6qJuIRLmQQmDjxo2ceWbLb70DBgzgs88+a5eiIt7unQBY2bqBTkSiW0ghkJCQQENDQ4ttDQ0NxDn0CZfGWxL8QWcCIhLlQgqBnJwcnnrqKerq6gCoq6vj6aef5txzz23P2iKXtwQS3JDRze5KREROSUgXhm+66SYee+wxbr31VpKSkqipqeHcc8/lzjvvbO/6IpLZtQNOP0NLO4pI1AspBJKSkrj33nupqKigrKwMj8dDampqO5cWwbw7sc482+4qRERO2QlNck9LSyM1NRVjDIFAAACXK6QRpZhh6mqholTXA0QkJoQUAuXl5Tz99NNs2rSJ2traFq8tWrSoXQqLWAcvCltZmhkkItEvpK/xTz31FPHx8dx333106NCBhx9+mKFDh3L77be3d30R5/DMoO72FiIi0gZCCoEtW7YwadIkevXqhWVZ9OrVi0mTJrFkyZL2ri/yeEsgsQN06Wp3JSIipyykEHC5XM33BHTq1Inq6moSExMpLy9v1+IikfGWwOndsRx2LUREYlNI1wT69evH+vXrGT58ODk5OcydOxe3203fvn3bu77I4y3BGny+3VWIiLSJkELgzjvvxBgDwPjx43nrrbeor6/n6quvbtfiIo2pqYaqCsjWzCARiQ3HDYFAIMDChQuZOHEiAG63m5/+9KcnvKMlS5awfPlyLMuie/fuTJ48mUWLFrF27Vri4+Pp1q0bkydPplOnTifeinBpnhmkEBCR2HDcgW2Xy8Unn3xySqtmlZeXs3TpUmbMmMHs2bMJBAKsWrWKIUOGMHv2bGbNmsXpp5/Oa6+9dtL7CAc9M0hEYk1IVzevvvpqFi9ejN/vP+kdBQIBfD4fTU1N+Hw+0tLSyMnJab7gPGDAgMi/0LyrBE7rCGkeuysREWkTIV0TeOedd6isrOTtt98mOTm5xWt//vOfj/v+9PR0xo4dy6RJk3C73eTk5JCTk9Pib5YvX84FF1zQ6vsLCgooKCgAYMaMGXg89nTC5ft2Q48+pGdknPB74+Pjbau7LcVKO0BtiVSx0pZoaUfIF4ZPRU1NDUVFRcybN4+OHTsyZ84cVq5cycUXXwzA3//+d+Li4hg1alSr78/LyyMvL6/599LS0lOq52QYYwjs2IZ13siT2r/H47Gl7rYWK+0AtSVSxUpbIq0dWVlZrW4PKQQGDRp0SjsvLi6ma9euzWcRI0aMYMuWLVx88cWsWLGCtWvXct99953SdYd2t78SavbreoCIxJSQQuC7ng903XXXHff9Ho+HrVu30tjYiNvtpri4mL59+7JhwwbeeOMN/vd//5fExMTQq7bDLs0MEpHYE1IIlJWVtfi9srKSjRs3Mnz48JB20r9/f3Jzc5k2bRpxcXH06tWLvLw87r77bvx+Pw8++GDz302YMOEEmxAezTODtKSkiMSQkEJg8uTJR23bsGED77//fsg7ys/PJz8/v8W2xx57LOT3285bAp06Q3Kq3ZWIiLSZk34AzpAhQygqKmrLWiKa8ZZAdo/Ivm4hInKCQjoT2LNnT4vfGxsbef/996Ni+lNbMMbArhKsERfbXYqISJsKKQTuuuuuFr+73W569+7NlClT2qWoiFNZDvW1mhkkIjHnlGcHOYJWExORGBXSNYGvvvrqqJseSktL+eqrr9qjpohjdu0I/qAzARGJMSGFwGOPPUZTU1OLbX6/n8cff7xdioo43hLonILVOfn4fysiEkVCCoHS0lK6devWYltmZib79u1rl6IiTXBmkIaCRCT2hBQC6enpfPHFFy22ffHFF6SlpbVLUZHEGAPenbpTWERiUkgXhq+++mpmzpzJj370I7p168aePXt46623uOaaa9q7PvuV74PGeq0mJiIxKaQQyMvLo1OnTixfvpyysjK6dOnCTTfdRG5ubnvXZ7+DF4V1JiAisSikEAAYOXIkI0eObM9aIpJWExORWBbSNYEFCxawefPmFts2b97MM8880x41RRZvCaR2weqYZHclIiJtLqQQ+OCDD+jbt2+LbX369DmhB8hFK7OrRGcBIhKzQgoBy7IIBAIttgUCgeDMmRhmAk3wjWYGiUjsCikEBg4cyMsvv9wcBIFAgMWLFzNw4MB2Lc52pXvA59PMIBGJWSFdGL7llluYMWMGEydObF43My0tjWnTprV3ffbyajUxEYltIYVAly5dePjhh9m2bRtlZWWkpKRQVFTEb3/7W/7yl7+0d422MbsOzQzqbm8hIiLtJOQpojU1NWzbto0VK1awY8cOzjrrLMaPH9+OpUUAbwl06YrVoaPdlYiItIvvDAG/38+aNWtYsWIFH3/8MZmZmVx44YWUlpYydepUUlJSwlWnLYxXM4NEJLZ9ZwjcfvvtuFwuRo8eTX5+Pn369AHgvffeC0txdjJNTfDN11iDz7e7FBGRdvOds4N69uxJbW0t27ZtY/v27dTU1ISrLvvt3Q1+v84ERCSmfeeZwAMPPMC+ffsoLCzkrbfeYuHChQwZMoTGxsaj1heIOYdmBml6qIjEsONeGM7IyGDcuHGMGzeOzz//nMLCQizL4p577uGSSy7hhhtuCEedYWd27QDLgkzNDBKR2BXy7CAI3jQ2cOBAbrnlFlavXs3KlSvbqy77eUvA0w0rMdHuSkRE2s0JhcAhbrebiy66iIsuuqit64kYWk1MRJwgpMdGOI3xH4C9Xt0pLCIxTyHQmj1eaGrSzCARiXkKgVYYzQwSEYdQCLRm1w5wuaDbGXZXIiLSrk7qwvDJWLJkCcuXL8eyLLp3787kyZPx+XzMnTuXffv2kZGRwdSpU0lKsn8FL+Mtga6nYyUk2F2KiEi7CsuZQHl5OUuXLmXGjBnMnj2bQCDAqlWreP311znnnHN49NFHOeecc3j99dfDUc7x7SqBLM0MEpHYF7bhoEAggM/no6mpCZ/PR1paGkVFRYwePRqA0aNHU1RUFK5yjskc8MG+bzQzSEQcISzDQenp6YwdO5ZJkybhdrvJyckhJyeHqqoq0tLSAEhLS6O6urrV9xcUFFBQUADAjBkz8Hg87VbrgS+3UG4CJA88mw5tuJ/4+Ph2rTtcYqUdoLZEqlhpS7S0IywhUFNTQ1FREfPmzaNjx47MmTPnhO42zsvLIy8vr/n30tLS9igTgMBnHwOwv3MqNW24n0MrskW7WGkHqC2RKlbaEmntyMrKanV7WIaDiouL6dq1K8nJycTHxzNixAi2bNlCSkoKFRUVAFRUVJCcnByOcr6btwTi4qFr6/9gIiKxJCwh4PF42Lp1K42NjRhjKC4uJjs7m6FDh1JYWAhAYWEhw4YNC0c538l4d0JmNlZ82CZOiYjYJiw9Xf/+/cnNzWXatGnExcXRq1cv8vLyaGhoYO7cuSxfvhyPx8Pdd98djnK+264dWL0H2F2FiEhYhO3rbn5+Pvn5+S22JSQkcN9994WrhOMyjQ1QugcuvMzuUkREwkJ3DB/JuxMAS/cIiIhDKASOcOiZQXpwnIg4hULgSN4SiE+Arpl2VyIiEhYKgSMY7w44/QwsV5zdpYiIhIVC4EjeEj0uQkQcRSFwkKmvg/JSLSkpIo6iEDjk0EIyOhMQEQdRCBykmUEi4kQKgUN27QB3InTpanclIiJhoxA4yHhL4PTuWC79k4iIc6jHO8S7E0sXhUXEYRQCgKndD1Xluh4gIo6jEIDgmsJoZpCIOI9CgCNmBmUrBETEWRQCAN4dcFpHSIv89UBFRNqSQoCDq4ll9cCyLLtLEREJK4UABFcT0/UAEXEgx4eAqa6EmmrI6m53KSIiYef4EDj8zCDdIyAizuP4EDC79MwgEXEux4cA3hLomAQpaXZXIiISdo4PAeMtgWzNDBIRZ3J0CBhjwKuZQSLiXI4OAarKoa5Wq4mJiGM5OwT0zCARcThHh4BWExMRp3N0COAtgc4pWJ1T7K5ERMQWjg4Bs2uHzgJExNEcGwLGGNi9U9cDRMTR4sOxE6/Xy9y5c5t/37t3L/n5+QwePJi//vWv+Hw+4uLiuO222+jXr184SoLyUmio18wgEXG0sIRAVlYWM2fOBCAQCDBx4kSGDx/OX/7yF8aNG8d5553HunXreOGFF3jggQfCUVJwDQE0M0hEnC3sw0HFxcVkZmaSkZGBZVnU19cDUFdXR1pa+B7doJlBIiJgGWNMOHf4xBNP0KdPH6666iq+/vprHnroISB4hjB9+nQyMjKOek9BQQEFBQUAzJgxA5/Pd8p1VD06Hd/Hq8l4+s1T/qxQxMfH4/f7w7Kv9hQr7QC1JVLFSlsirR1ut7vV7WEZDjrE7/ezdu1afvGLXwDw3nvvcfPNN5Obm8uqVat48skn+cMf/nDU+/Ly8sjLy2v+vbS09JRrafpiC2Se0SafFQqPxxO2fbWnWGkHqC2RKlbaEmntyMrKanV7WIeD1q9fT+/evUlNTQWgsLCQESNGADBy5Ei2bdsWljpMIAC7S7SGgIg4XlhD4IMPPuDCCy9s/j09PZ2NGzcC8Omnn5KZmRmeQkr3gM+n1cRExPHCNhzU2NjIJ598woQJE5q3TZw4kYULFxIIBEhISGDixInhKcarZwaJiEAYQyAxMZEFCxa02DZw4EAefvjhcJXQTDODRESCnHnH8K4SSM/AOq2j3ZWIiNjKkSEQXE1MF4VFRBwXAqapCb75GksXhUVEnBcC7NsN/gO6HiAighND4NDMIA0HiYg4LwTMrhKwLMjUcJCIiONCAG8JeLphJSbaXYmIiO0cFwLGW6LrASIiBzkqBIz/AOzZpTuFRUQOclQIsGc3NDXpHgERkYMcFQJGzwwSEWnBUSGAdwdYLsjMtrsSEZGI4KgQMN4S6HY6VkLrK+yIiDiNo0KAXZoZJCJyJMeEgDngg727dT1AROQIjgkBvtkFJgBaUlJEpJljQsDs2gFoZpCIyJEcEwJ4SyAuHrqdbnclIiIRwzkhkJGJNfISrPgEuysREYkYYVtj2G6uUVfAqCvsLkNEJKI450xARESOohAQEXEwhYCIiIMpBEREHEwhICLiYAoBEREHUwiIiDiYQkBExMEsY4yxuwgREbGHzgTC5De/+Y3dJbSJWGkHqC2RKlbaEi3tUAiIiDiYQkBExMEUAmGSl5dndwltIlbaAWpLpIqVtkRLO3RhWETEwXQmICLiYAoBEREHc8yiMnaZMmUKHTp0wOVyERcXx4wZM+wuKWRPPPEE69atIyUlhdmzZwNQU1PD3Llz2bdvHxkZGUydOpWkpCSbKz2+1tqyePFili1bRnJyMgA///nPOf/88+0s87hKS0uZN28elZWVWJZFXl4eY8aMicrjcqy2RONx8fl83H///fj9fpqamsjNzSU/Pz86jouRdjV58mRTVVVldxkn5bPPPjPbt283d999d/O2559/3rz22mvGGGNee+018/zzz9tU3YlprS2LFi0yb7zxho1Vnbjy8nKzfft2Y4wxdXV15q677jI7d+6MyuNyrLZE43EJBAKmvr7eGGPMgQMHzL333ms2b94cFcdFw0FyTIMGDTrqW0tRURGjR48GYPTo0RQVFdlR2glrrS3RKC0tjT59+gBw2mmnkZ2dTXl5eVQel2O1JRpZlkWHDh0AaGpqoqmpCcuyouK4aDgoDB566CEALr/88qiZNnYsVVVVpKWlAcH/iaurq22u6NS8++67rFy5kj59+nDTTTdFVVDs3buXL7/8kn79+kX9cTmyLZ9//nlUHpdAIMC0adP45ptvuPLKK+nfv39UHBeFQDt78MEHSU9Pp6qqiunTp5OVlcWgQYPsLkuAK664gnHjxgGwaNEinnvuOSZPnmxzVaFpaGhg9uzZjB8/no4dO9pdzin5dlui9bi4XC5mzpxJbW0ts2bNoqSkxO6SQqLhoHaWnp4OQEpKCsOGDWPbtm02V3RqUlJSqKioAKCioqL54l00Sk1NxeVy4XK5uOyyy9i+fbvdJYXE7/cze/ZsRo0axYgRI4DoPS6ttSVaj8shnTp1YtCgQWzYsCEqjotCoB01NDRQX1/f/PMnn3xCjx49bK7q1AwdOpTCwkIACgsLGTZsmM0VnbxD/3MCrF69mu7du9tYTWiMMTz55JNkZ2fzwx/+sHl7NB6XY7UlGo9LdXU1tbW1QHCmUHFxMdnZ2VFxXHTHcDvas2cPs2bNAoIXiy666CKuueYam6sK3SOPPMLGjRvZv38/KSkp5OfnM2zYMObOnUtpaSkej4e77747KsZrW2vLZ599xldffYVlWWRkZDBhwoTm8dtI9fnnn3PffffRo0cPLMsCglMo+/fvH3XH5Vht+eCDD6LuuOzYsYN58+YRCAQwxjBy5EjGjRvH/v37I/64KARERBxMw0EiIg6mEBARcTCFgIiIgykEREQcTCEgIuJgCgGRMCotLeXGG28kEAjYXYoIoCmi4hBTpkyhsrISl8tFfHw8AwYM4Pbbb8fj8bT5vsrKyli4cCGbNm3C7/fj8XgYO3Ys3//+99t8XyKnSs8OEseYNm0aQ4YMwefzMX/+fBYsWMCvf/3rNt/P448/Ts+ePZk3bx4JCQmUlJRQWVnZ5vsRaQsKAXEct9tNbm4uzz77bPO2devW8fLLL7Nnzx46duzIJZdcQn5+fvPrhYWFLFq0iIaGBsaMGcO//vUvJk6cyJAhQ476/G3btnHzzTc3P1q4d+/eza/t3buXO+64g5deeont27fz4IMPNr8WCARITU1tvvP0zTffZNmyZdTW1nL22WczYcKEiLvbVKKfQkAcp7GxkVWrVtG/f//mbYmJidxxxx2cccYZ7Ny5k+nTp9OrVy+GDx/O119/zfz58/nd735Hv379ePHFF7/zufcDBgzg6aef5qqrruLMM8885pDTgAEDeP7554Hgg9SmT5/OgAEDAFi6dClFRUU88MADJCcns3DhQubPn88vf/nLtvuHEEEhIA4yc+ZM4uLiaGhoICUlhd/97nfNrw0ePLj55549e3LhhReyceNGhg8fzkcffcT3vvc9Bg4cCMB1113H0qVLj7mfqVOn8sYbb/Dqq6+ya9cuevTowcSJE+nXr98x37Nw4UISExP52c9+BkBBQQG33norXbp0AeDaa69l8uTJNDU1ERcXd0r/DiJHUgiIY9xzzz0MGTKEQCBAUVER999/P3PnziU1NZWtW7fy4osvUlJSgt/vx+/3k5ubC0B5eXmLb/OJiYl07tz5mPtJSkri+uuv5/rrr6e6uprnn3+emTNn8uSTT7b69//85z/ZuHEjDz30EC5XcMLevn37mDVrVvOD1SD4vPqqqqrmx5OLtAWFgDiOy+VixIgRPPXUU3z++efk5uby6KOPcuWVV3Lvvffidrt55plnmleBSktLw+v1Nr/f5/Oxf//+kPaVnJzM2LFjKSwspKam5qjXN23axKJFi/jjH//YYnGYLl26MGnSpOazD5H2ovsExHGMMRQVFVFbW0t2djYA9fX1JCUl4Xa72bZtG++//37z3+fm5rJ27Vo2b96M3+9n8eLF3/n5L7zwAiUlJTQ1NVFfX897771HZmbmUWcPpaWlPPLII9xxxx1kZWW1eO3yyy/n5ZdfZt++fUDwefWRuD6tRD+dCYhjPPzww7hcrubn1E+ZMqV5wZLbbruN5557jgULFjBo0CBGjhzZvEhI9+7dufXWW3nkkUdobGxkzJgxJCcnk5CQ0Op+fD4fs2bNoqKiArfbTf/+/Vudivrpp59SWVnJ7Nmzm7dlZGQwZ84cxowZA8D06dOpqKggJSWFkSNHRuSiJBLddLOYyAlqaGhg/PjxPProo3Tt2tXuckROiYaDREKwZs0aGhsbaWho4LnnnqNHjx5kZGTYXZbIKdNwkEgI1qxZw+OPP44xhr59+/LLX/6yxcwdkWil4SAREQfTcJCIiIMpBEREHEwhICLiYAoBEREHUwiIiDjY/wfgkpMxbghm3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "# load the MNIST dataset\n",
    "input_size  = 28*28   # images are 28x28 pixels\n",
    "output_size = 10      # there are 10 classes\n",
    "'''\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "'''\n",
    "\n",
    "        \n",
    "training_data = datasets.MNIST('./data/MNIST', train=True, download=True,\n",
    "           transform=transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               transforms.Normalize((0.1307,), (0.3081,))\n",
    "           ]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True)\n",
    "\n",
    "#Create paths to store trained models\n",
    "TRAINING_PATH = \"./training/\"\n",
    "try:\n",
    "    os.mkdir(TRAINING_PATH)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "'''\n",
    "# define function for weight initialization\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.zeros_(m.bias)\n",
    "'''\n",
    "\n",
    "# define function to create bags for training:\n",
    "def bagging():\n",
    "    bag = list()\n",
    "    while len(bag) < len(training_data):\n",
    "        index = np.random.randint(len(training_data))\n",
    "        bag.append(training_data[index])\n",
    "    return bag\n",
    "\n",
    "# define the multi-layer perceptron model\n",
    "class FC2Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FC2Layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, output_size), \n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)\n",
    "    \n",
    "\n",
    "# define the training loop\n",
    "def train(epoch, model, k):\n",
    "    for i in range(k):\n",
    "        #initiailize weight\n",
    "        #model.apply(weights_init)\n",
    "        # initialize the optimizer\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=moment)\n",
    "        print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "        model.train()\n",
    "        #Bootstrap training data\n",
    "        training_data = bagging()\n",
    "        train_loader = torch.utils.data.DataLoader(training_data,batch_size=64, shuffle=True)\n",
    "        print('Training... k = {}'.format(k))\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass through the model\n",
    "            output = model(data)\n",
    "            # forward pass through the cross-entropy loss function\n",
    "            loss = F.nll_loss(output, target)\n",
    "            # backward pass through the cross-entropy loss function and the model\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Model {} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(i+1,\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "        #Save inidvidual models of an emsemble\n",
    "        torch.save(model.state_dict(), TRAINING_PATH + 'model{}_state_dict'.format(i+1) + \".pt\")\n",
    "\n",
    "# define the testing loop\n",
    "ensemble_accuracy_container = []\n",
    "def test(model, k):\n",
    "    for i in range(k):\n",
    "        #load individual models of an ensemble \n",
    "        model.load_state_dict(torch.load(TRAINING_PATH + 'model{}_state_dict'.format(i+1) + \".pt\"))\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss \n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print('\\nModel {} Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(i+1,\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            accuracy))\n",
    "    \n",
    "    #ensemble accuracy\n",
    "    ensemble_correct = 0\n",
    "    for data, target in test_loader:\n",
    "        ensemble_predict = np.zeros((1000,1)) #placeholder\n",
    "        for i in range(k):\n",
    "            model.load_state_dict(torch.load(TRAINING_PATH + 'model{}_state_dict'.format(i+1) + \".pt\"))\n",
    "            model.eval()\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1] \n",
    "            np_pred = pred.numpy()\n",
    "            ensemble_predict = np.hstack((ensemble_predict, np_pred))\n",
    "        #MAJORITY VOTING OF DIFFERENT MODELS ON EACH BATCH\n",
    "        ensemble_predict = ensemble_predict[:,1:]\n",
    "        #VOTE:\n",
    "        ensemble_predict = torch.from_numpy(stats.mode(ensemble_predict, axis = 1)[0])\n",
    "        ensemble_correct += ensemble_predict.eq(target.data.view_as(ensemble_predict)).cpu().sum().item()\n",
    "    ensemble_accuracy = 100. * ensemble_correct / len(test_loader.dataset)\n",
    "    print(\"Accuracy of Ensemble of networks with {} models: {}/{} ({:.0f}%)\\n\".format(k, ensemble_correct, \n",
    "            len(test_loader.dataset),\n",
    "            ensemble_accuracy))\n",
    "    ensemble_accuracy_container.append(ensemble_accuracy)\n",
    "\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np\n",
    "    \n",
    "# build the model and execute the training and testing\n",
    "\n",
    "# initialize some hyper-paramters \n",
    "n_hidden = 4 # number of hidden layers\n",
    "learning_rate = 0.01\n",
    "moment = 0.5\n",
    "nepochs = 1\n",
    "\n",
    "k_list = [2,4,8,16,32]\n",
    "\n",
    "# build the actual model\n",
    "np.random.seed(10003) \n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "\n",
    "# train the model for one epoch\n",
    "for k in k_list:\n",
    "    for epoch in range(0, nepochs):\n",
    "        train(epoch, model_fnn, k)\n",
    "        test(model_fnn,k)\n",
    "        \n",
    "#plot:\n",
    "plt.style.use('ggplot')\n",
    "plt.xlabel('Bag Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(k_list, ensemble_accuracy_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "374px",
    "left": "1310px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
